{
  "worker_id": 1,
  "success": true,
  "kernel_code": "# kernel.py\n# Triton-based implementation of a U-Net-like forward with DoubleConv blocks (Conv3x3 + BN + Softmax)\n# All math is done inside Triton kernels; the Python wrapper only allocates and launches.\nimport torch\nimport triton\nimport triton.language as tl\n\n\n# ============================\n# 3x3 Conv + BN + Softmax (dim=-1)\n# ============================\n@triton.jit\ndef _conv3_bn_sm_row_kernel(\n    x_ptr, w_ptr, b_ptr, gamma_ptr, beta_ptr, mean_ptr, var_ptr, y_ptr,\n    N, Cin, H, W, Cout,\n    sXN, sXC, sXH, sXW,\n    sWO, sWI, sWKH, sWKW,\n    sYN, sYC, sYH, sYW,\n    EPS: tl.constexpr,\n    BLOCK_W: tl.constexpr,\n):\n    pid_w = tl.program_id(axis=0)\n    pid_r = tl.program_id(axis=1)\n\n    row_size = Cout * H\n    n = pid_r // row_size\n    rem = pid_r % row_size\n    oc = rem // H\n    h = rem % H\n\n    w_start = pid_w * BLOCK_W\n    offs_w = w_start + tl.arange(0, BLOCK_W)\n    mask_w = offs_w < W\n\n    acc = tl.zeros((BLOCK_W,), dtype=tl.float32)\n\n    # bias load\n    b = tl.load(b_ptr + oc).to(tl.float32)\n\n    # 3x3 conv with padding=1\n    for ic in tl.range(0, Cin):\n        for kh in range(3):\n            ih = h + kh - 1\n            h_ok = (ih >= 0) & (ih < H)\n            for kw in range(3):\n                iw = offs_w + kw - 1\n                m = mask_w & h_ok & (iw >= 0) & (iw < W)\n                x_ptrs = x_ptr + n * sXN + ic * sXC + ih * sXH + iw * sXW\n                x_vals = tl.load(x_ptrs, mask=m, other=0.0).to(tl.float32)\n                w_val = tl.load(w_ptr + (oc * sWO + ic * sWI + kh * sWKH + kw * sWKW)).to(tl.float32)\n                acc += x_vals * w_val\n\n    acc += b\n\n    # BatchNorm2d (eval): y = (x - mean) / sqrt(var + eps) * gamma + beta\n    mean = tl.load(mean_ptr + oc).to(tl.float32)\n    var = tl.load(var_ptr + oc).to(tl.float32)\n    gamma = tl.load(gamma_ptr + oc).to(tl.float32)\n    beta = tl.load(beta_ptr + oc).to(tl.float32)\n    inv_std = 1.0 / tl.sqrt(var + EPS)\n    bn_out = (acc - mean) * inv_std\n    bn_out = bn_out * gamma + beta\n\n    # Softmax along last dim (width)\n    # Stability: subtract row max\n    neg_large = tl.full((BLOCK_W,), -1.0e30, dtype=tl.float32)\n    masked_vals = tl.where(mask_w, bn_out, neg_large)\n    row_max = tl.max(masked_vals, axis=0)\n    shifted = masked_vals - row_max\n    ex = tl.exp(shifted)\n    ex = tl.where(mask_w, ex, 0.0)\n    denom = tl.sum(ex, axis=0)\n    sm = ex / denom\n\n    y_ptrs = y_ptr + n * sYN + oc * sYC + h * sYH + offs_w * sYW\n    tl.store(y_ptrs, sm.to(y_ptr.dtype.element_ty), mask=mask_w)\n\n\ndef conv3_bn_softmax(x, w, b, gamma, beta, running_mean, running_var, eps=1e-5, block_w=512):\n    # x: [N,Cin,H,W], w: [Cout,Cin,3,3]\n    assert x.is_cuda and w.is_cuda\n    N, Cin, H, W = x.shape\n    Cout = w.shape[0]\n    y = torch.empty((N, Cout, H, W), device=x.device, dtype=x.dtype)\n\n    grid = (triton.cdiv(W, block_w), N * Cout * H)\n    _conv3_bn_sm_row_kernel[grid](\n        x, w, b, gamma, beta, running_mean, running_var, y,\n        N, Cin, H, W, Cout,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        w.stride(0), w.stride(1), w.stride(2), w.stride(3),\n        y.stride(0), y.stride(1), y.stride(2), y.stride(3),\n        EPS=eps, BLOCK_W=block_w\n    )\n    return y\n\n\n# ============================\n# 1x1 Conv (no BN / no Softmax)\n# ============================\n@triton.jit\ndef _conv1x1_row_kernel(\n    x_ptr, w_ptr, b_ptr, y_ptr,\n    N, Cin, H, W, Cout,\n    sXN, sXC, sXH, sXW,\n    sWO, sWI, sWKH, sWKW,  # still pass 4D strides for consistency (KH=KW=1)\n    sYN, sYC, sYH, sYW,\n    BLOCK_W: tl.constexpr,\n):\n    pid_w = tl.program_id(axis=0)\n    pid_r = tl.program_id(axis=1)\n\n    row_size = Cout * H\n    n = pid_r // row_size\n    rem = pid_r % row_size\n    oc = rem // H\n    h = rem % H\n\n    w_start = pid_w * BLOCK_W\n    offs_w = w_start + tl.arange(0, BLOCK_W)\n    mask_w = offs_w < W\n\n    acc = tl.zeros((BLOCK_W,), dtype=tl.float32)\n    b = tl.load(b_ptr + oc).to(tl.float32)\n\n    for ic in tl.range(0, Cin):\n        x_ptrs = x_ptr + n * sXN + ic * sXC + h * sXH + offs_w * sXW\n        x_vals = tl.load(x_ptrs, mask=mask_w, other=0.0).to(tl.float32)\n        w_val = tl.load(w_ptr + (oc * sWO + ic * sWI)).to(tl.float32)\n        acc += x_vals * w_val\n\n    acc += b\n    y_ptrs = y_ptr + n * sYN + oc * sYC + h * sYH + offs_w * sYW\n    tl.store(y_ptrs, acc.to(y_ptr.dtype.element_ty), mask=mask_w)\n\n\ndef conv1x1(x, w, b, block_w=512):\n    # x: [N,Cin,H,W], w: [Cout,Cin,1,1]\n    N, Cin, H, W = x.shape\n    Cout = w.shape[0]\n    y = torch.empty((N, Cout, H, W), device=x.device, dtype=x.dtype)\n    grid = (triton.cdiv(W, block_w), N * Cout * H)\n    _conv1x1_row_kernel[grid](\n        x, w, b, y,\n        N, Cin, H, W, Cout,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        w.stride(0), w.stride(1), w.stride(2), w.stride(3),\n        y.stride(0), y.stride(1), y.stride(2), y.stride(3),\n        BLOCK_W=block_w\n    )\n    return y\n\n\n# ============================\n# MaxPool2d kernel_size=2, stride=2\n# ============================\n@triton.jit\ndef _maxpool2x2_row_kernel(\n    x_ptr, y_ptr,\n    N, C, H_in, W_in, H_out, W_out,\n    sXN, sXC, sXH, sXW,\n    sYN, sYC, sYH, sYW,\n    BLOCK_W: tl.constexpr,\n):\n    pid_w = tl.program_id(axis=0)\n    pid_r = tl.program_id(axis=1)\n\n    row_size = C * H_out\n    n = pid_r // row_size\n    rem = pid_r % row_size\n    c = rem // H_out\n    ho = rem % H_out\n\n    w_start = pid_w * BLOCK_W\n    offs_wo = w_start + tl.arange(0, BLOCK_W)\n    mask_w = offs_wo < W_out\n\n    hi0 = 2 * ho\n    wi0 = 2 * offs_wo\n\n    x00_ptrs = x_ptr + n * sXN + c * sXC + (hi0 + 0) * sXH + (wi0 + 0) * sXW\n    x01_ptrs = x_ptr + n * sXN + c * sXC + (hi0 + 0) * sXH + (wi0 + 1) * sXW\n    x10_ptrs = x_ptr + n * sXN + c * sXC + (hi0 + 1) * sXH + (wi0 + 0) * sXW\n    x11_ptrs = x_ptr + n * sXN + c * sXC + (hi0 + 1) * sXH + (wi0 + 1) * sXW\n\n    # Inside bounds guards not necessary for even shapes here, but keep mask for safety\n    valid = mask_w  # shapes are exact halves in the test\n    x00 = tl.load(x00_ptrs, mask=valid, other=-1.0e30).to(tl.float32)\n    x01 = tl.load(x01_ptrs, mask=valid, other=-1.0e30).to(tl.float32)\n    x10 = tl.load(x10_ptrs, mask=valid, other=-1.0e30).to(tl.float32)\n    x11 = tl.load(x11_ptrs, mask=valid, other=-1.0e30).to(tl.float32)\n\n    m0 = tl.maximum(x00, x01)\n    m1 = tl.maximum(x10, x11)\n    m = tl.maximum(m0, m1)\n\n    y_ptrs = y_ptr + n * sYN + c * sYC + ho * sYH + offs_wo * sYW\n    tl.store(y_ptrs, m.to(y_ptr.dtype.element_ty), mask=mask_w)\n\n\ndef maxpool2x2(x, block_w=512):\n    N, C, H_in, W_in = x.shape\n    assert H_in % 2 == 0 and W_in % 2 == 0\n    H_out, W_out = H_in // 2, W_in // 2\n    y = torch.empty((N, C, H_out, W_out), device=x.device, dtype=x.dtype)\n    grid = (triton.cdiv(W_out, block_w), N * C * H_out)\n    _maxpool2x2_row_kernel[grid](\n        x, y,\n        N, C, H_in, W_in, H_out, W_out,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        y.stride(0), y.stride(1), y.stride(2), y.stride(3),\n        BLOCK_W=block_w\n    )\n    return y\n\n\n# ============================\n# ConvTranspose2d k=2, stride=2, padding=0\n# For this configuration, each output element depends on exactly one kernel offset:\n# kh = h_out % 2, kw = w_out % 2, and input location is (h_out//2, w_out//2)\n# Weight layout: [Cin, Cout, 2, 2] (PyTorch)\n# ============================\n@triton.jit\ndef _deconv2x2s2_row_kernel(\n    x_ptr, w_ptr, b_ptr, y_ptr,\n    N, Cin, H_in, W_in, Cout, H_out, W_out,\n    sXN, sXC, sXH, sXW,\n    sWO, sWI, sWKH, sWKW,  # For convtranspose: O==Cin, I==Cout here (using generic names)\n    sYN, sYC, sYH, sYW,\n    BLOCK_W: tl.constexpr,\n):\n    pid_w = tl.program_id(axis=0)\n    pid_r = tl.program_id(axis=1)\n\n    row_size = Cout * H_out\n    n = pid_r // row_size\n    rem = pid_r % row_size\n    oc = rem // H_out\n    ho = rem % H_out\n\n    w_start = pid_w * BLOCK_W\n    offs_wo = w_start + tl.arange(0, BLOCK_W)\n    mask_w = offs_wo < W_out\n\n    # map output positions to input positions (h_out -> h_in, w_out -> w_in)\n    hi = ho // 2\n    kh = ho % 2\n    wi = offs_wo // 2\n    kw_mask = (offs_wo % 2) == 1  # kw=1 if True else 0\n\n    acc = tl.zeros((BLOCK_W,), dtype=tl.float32)\n    b = tl.load(b_ptr + oc).to(tl.float32)\n    acc += b\n\n    for ic in tl.range(0, Cin):\n        x_ptrs = x_ptr + n * sXN + ic * sXC + hi * sXH + wi * sXW\n        x_vals = tl.load(x_ptrs, mask=mask_w, other=0.0).to(tl.float32)\n        # two possible kw per lane; pick via kw_mask\n        w0 = tl.load(w_ptr + (ic * sWO + oc * sWI + kh * sWKH + 0 * sWKW)).to(tl.float32)\n        w1 = tl.load(w_ptr + (ic * sWO + oc * sWI + kh * sWKH + 1 * sWKW)).to(tl.float32)\n        wv = tl.where(kw_mask, w1, w0)\n        acc += x_vals * wv\n\n    y_ptrs = y_ptr + n * sYN + oc * sYC + ho * sYH + offs_wo * sYW\n    tl.store(y_ptrs, acc.to(y_ptr.dtype.element_ty), mask=mask_w)\n\n\ndef deconv2x2s2(x, w, b, block_w=512):\n    # x: [N,Cin,H_in,W_in], w: [Cin,Cout,2,2]\n    N, Cin, H_in, W_in = x.shape\n    Cout = w.shape[1]\n    H_out, W_out = H_in * 2, W_in * 2\n    y = torch.empty((N, Cout, H_out, W_out), device=x.device, dtype=x.dtype)\n    grid = (triton.cdiv(W_out, block_w), N * Cout * H_out)\n    _deconv2x2s2_row_kernel[grid](\n        x, w, b, y,\n        N, Cin, H_in, W_in, Cout, H_out, W_out,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        w.stride(0), w.stride(1), w.stride(2), w.stride(3),\n        y.stride(0), y.stride(1), y.stride(2), y.stride(3),\n        BLOCK_W=block_w\n    )\n    return y\n\n\n# ============================\n# Channel-wise Concatenation via Copy with Channel Offset\n# ============================\n@triton.jit\ndef _cat_copy_row_kernel(\n    src_ptr, dst_ptr,\n    N, C, H, W, c_offset,\n    sSN, sSC, sSH, sSW,\n    sDN, sDC, sDH, sDW,\n    BLOCK_W: tl.constexpr,\n):\n    pid_w = tl.program_id(axis=0)\n    pid_r = tl.program_id(axis=1)\n\n    row_size = C * H\n    n = pid_r // row_size\n    rem = pid_r % row_size\n    c = rem // H\n    h = rem % H\n\n    w_start = pid_w * BLOCK_W\n    offs_w = w_start + tl.arange(0, BLOCK_W)\n    mask_w = offs_w < W\n\n    src_ptrs = src_ptr + n * sSN + c * sSC + h * sSH + offs_w * sSW\n    vals = tl.load(src_ptrs, mask=mask_w, other=0.0)\n    dst_ptrs = dst_ptr + n * sDN + (c + c_offset) * sDC + h * sDH + offs_w * sDW\n    tl.store(dst_ptrs, vals, mask=mask_w)\n\n\ndef cat_channels(x1, x2, block_w=512):\n    # Concatenate along channel dimension (N, C1 + C2, H, W) = cat([x1, x2], dim=1)\n    assert x1.shape[0] == x2.shape[0] and x1.shape[2:] == x2.shape[2:]\n    N, C1, H, W = x1.shape\n    C2 = x2.shape[1]\n    y = torch.empty((N, C1 + C2, H, W), device=x1.device, dtype=x1.dtype)\n\n    grid1 = (triton.cdiv(W, block_w), N * C1 * H)\n    _cat_copy_row_kernel[grid1](\n        x1, y,\n        N, C1, H, W, 0,\n        x1.stride(0), x1.stride(1), x1.stride(2), x1.stride(3),\n        y.stride(0), y.stride(1), y.stride(2), y.stride(3),\n        BLOCK_W=block_w\n    )\n    grid2 = (triton.cdiv(W, block_w), N * C2 * H)\n    _cat_copy_row_kernel[grid2](\n        x2, y,\n        N, C2, H, W, C1,\n        x2.stride(0), x2.stride(1), x2.stride(2), x2.stride(3),\n        y.stride(0), y.stride(1), y.stride(2), y.stride(3),\n        BLOCK_W=block_w\n    )\n    return y\n\n\n# ============================\n# Helpers to extract parameters from the provided state_dict\n# ============================\ndef _dc_params(state, prefix):\n    # DoubleConv params under f\"{prefix}.double_conv\"\n    p = f\"{prefix}.double_conv\"\n    conv1_w = state[f\"{p}.0.weight\"]\n    conv1_b = state[f\"{p}.0.bias\"]\n    bn1_w = state[f\"{p}.1.weight\"]\n    bn1_b = state[f\"{p}.1.bias\"]\n    bn1_mean = state[f\"{p}.1.running_mean\"]\n    bn1_var = state[f\"{p}.1.running_var\"]\n\n    conv2_w = state[f\"{p}.3.weight\"]\n    conv2_b = state[f\"{p}.3.bias\"]\n    bn2_w = state[f\"{p}.4.weight\"]\n    bn2_b = state[f\"{p}.4.bias\"]\n    bn2_mean = state[f\"{p}.4.running_mean\"]\n    bn2_var = state[f\"{p}.4.running_var\"]\n\n    return (conv1_w, conv1_b, bn1_w, bn1_b, bn1_mean, bn1_var,\n            conv2_w, conv2_b, bn2_w, bn2_b, bn2_mean, bn2_var)\n\n\ndef _up_params(state, name):\n    # ConvTranspose2d params: weight [Cin, Cout, 2, 2], bias [Cout]\n    w = state[f\"{name}.weight\"]\n    b = state[f\"{name}.bias\"]\n    return w, b\n\n\ndef _final_params(state):\n    w = state[\"final_conv.weight\"]\n    b = state[\"final_conv.bias\"]\n    return w, b\n\n\n# ============================\n# Orchestration: forward pass\n# ============================\ndef _double_conv_block(x, params, eps=1e-5):\n    (w1, b1, g1, be1, m1, v1,\n     w2, b2, g2, be2, m2, v2) = params\n    y = conv3_bn_softmax(x, w1, b1, g1, be1, m1, v1, eps=eps)\n    y = conv3_bn_softmax(y, w2, b2, g2, be2, m2, v2, eps=eps)\n    return y\n\n\ndef _forward_unet(x, state):\n    # Encoder\n    enc1 = _double_conv_block(x, _dc_params(state, \"encoder1\"))\n    p1 = maxpool2x2(enc1)\n    enc2 = _double_conv_block(p1, _dc_params(state, \"encoder2\"))\n    p2 = maxpool2x2(enc2)\n    enc3 = _double_conv_block(p2, _dc_params(state, \"encoder3\"))\n    p3 = maxpool2x2(enc3)\n    enc4 = _double_conv_block(p3, _dc_params(state, \"encoder4\"))\n    p4 = maxpool2x2(enc4)\n\n    # Bottleneck\n    bottleneck = _double_conv_block(p4, _dc_params(state, \"bottleneck\"))\n\n    # Decoder\n    up4_w, up4_b = _up_params(state, \"upconv4\")\n    dec4 = deconv2x2s2(bottleneck, up4_w, up4_b)\n    dec4 = cat_channels(dec4, enc4)\n    dec4 = _double_conv_block(dec4, _dc_params(state, \"decoder4\"))\n\n    up3_w, up3_b = _up_params(state, \"upconv3\")\n    dec3 = deconv2x2s2(dec4, up3_w, up3_b)\n    dec3 = cat_channels(dec3, enc3)\n    dec3 = _double_conv_block(dec3, _dc_params(state, \"decoder3\"))\n\n    up2_w, up2_b = _up_params(state, \"upconv2\")\n    dec2 = deconv2x2s2(dec3, up2_w, up2_b)\n    dec2 = cat_channels(dec2, enc2)\n    dec2 = _double_conv_block(dec2, _dc_params(state, \"decoder2\"))\n\n    up1_w, up1_b = _up_params(state, \"upconv1\")\n    dec1 = deconv2x2s2(dec2, up1_w, up1_b)\n    dec1 = cat_channels(dec1, enc1)\n    dec1 = _double_conv_block(dec1, _dc_params(state, \"decoder1\"))\n\n    # Final 1x1 conv to out_channels\n    fw, fb = _final_params(state)\n    out = conv1x1(dec1, fw, fb)\n    return out\n\n\n# ============================\n# Public API\n# ============================\ndef kernel_function(x, state_dict):\n    \"\"\"\n    Execute the full forward pass of the provided U-Net-like model using Triton kernels.\n\n    Args:\n      x: Input tensor of shape [N, C_in, H, W], on CUDA, dtype bf16 preferred.\n      state_dict: A PyTorch state dict (tensors on same device/dtype) containing all model weights/buffers.\n\n    Returns:\n      Output tensor [N, out_channels, H, W], same device/dtype as x.\n    \"\"\"\n    # Validation\n    if not isinstance(x, torch.Tensor):\n        raise TypeError(\"x must be a torch.Tensor\")\n    if not x.is_cuda:\n        raise ValueError(\"x must be on CUDA device\")\n    if not x.is_contiguous():\n        x = x.contiguous()\n    # Basic dtype/device checks on state_dict\n    if not isinstance(state_dict, dict):\n        raise TypeError(\"state_dict must be a dict of tensors\")\n    dev = x.device\n    for k, v in state_dict.items():\n        if isinstance(v, torch.Tensor):\n            if v.device != dev:\n                raise ValueError(f\"State tensor {k} is on {v.device}, expected {dev}\")\n            # ensure contiguous\n            if not v.is_contiguous():\n                state_dict[k] = v.contiguous()\n\n    # Run the network\n    with torch.no_grad():\n        y = _forward_unet(x, state_dict)\n    return y",
  "rounds": 2,
  "history": [
    {
      "round": 1,
      "timestamp": "2025-12-04T09:50:11.512806",
      "success": false,
      "kernel_code": "\"\"\"\nKernel implementation 2.\n\"\"\"\n\ndef kernel_function(*args, **kwargs):\n    \"\"\"Wrapper function that handles kernel launch.\"\"\"\n    # Mock implementation that fails\n    raise NotImplementedError('Mock kernel not implemented')\n",
      "stdout": "All kernel_function calling attempts failed. Tried:\n  - kernel_function(x) -> NotImplementedError: Mock kernel not implemented\n  - kernel_function(x, state_dict) -> NotImplementedError: Mock kernel not implemented\n  - kernel_function(x, flat_params) -> NotImplementedError: Mock kernel not implemented\n  - kernel_function(x, in_ch, out_ch, features) -> NotImplementedError: Mock kernel not implemented\n  - kernel_function(in_ch, out_ch, features, x) -> NotImplementedError: Mock kernel not implemented\n  - kernel_function(x, state_dict, in_ch, out_ch, features) -> NotImplementedError: Mock kernel not implemented\n  - kernel_function(in_ch, out_ch, features, x, state_dict) -> NotImplementedError: Mock kernel not implemented\n  - kernel_function(x, flat_params, in_ch, out_ch, features) -> NotImplementedError: Mock kernel not implemented\n  - kernel_function({'x': x, 'state_dict': state, 'in_channels':..., ...}) -> NotImplementedError: Mock kernel not implemented\n",
      "stderr": ""
    },
    {
      "round": 2,
      "timestamp": "2025-12-04T10:01:01.054982",
      "success": true,
      "kernel_code": "# kernel.py\n# Triton-based implementation of a U-Net-like forward with DoubleConv blocks (Conv3x3 + BN + Softmax)\n# All math is done inside Triton kernels; the Python wrapper only allocates and launches.\nimport torch\nimport triton\nimport triton.language as tl\n\n\n# ============================\n# 3x3 Conv + BN + Softmax (dim=-1)\n# ============================\n@triton.jit\ndef _conv3_bn_sm_row_kernel(\n    x_ptr, w_ptr, b_ptr, gamma_ptr, beta_ptr, mean_ptr, var_ptr, y_ptr,\n    N, Cin, H, W, Cout,\n    sXN, sXC, sXH, sXW,\n    sWO, sWI, sWKH, sWKW,\n    sYN, sYC, sYH, sYW,\n    EPS: tl.constexpr,\n    BLOCK_W: tl.constexpr,\n):\n    pid_w = tl.program_id(axis=0)\n    pid_r = tl.program_id(axis=1)\n\n    row_size = Cout * H\n    n = pid_r // row_size\n    rem = pid_r % row_size\n    oc = rem // H\n    h = rem % H\n\n    w_start = pid_w * BLOCK_W\n    offs_w = w_start + tl.arange(0, BLOCK_W)\n    mask_w = offs_w < W\n\n    acc = tl.zeros((BLOCK_W,), dtype=tl.float32)\n\n    # bias load\n    b = tl.load(b_ptr + oc).to(tl.float32)\n\n    # 3x3 conv with padding=1\n    for ic in tl.range(0, Cin):\n        for kh in range(3):\n            ih = h + kh - 1\n            h_ok = (ih >= 0) & (ih < H)\n            for kw in range(3):\n                iw = offs_w + kw - 1\n                m = mask_w & h_ok & (iw >= 0) & (iw < W)\n                x_ptrs = x_ptr + n * sXN + ic * sXC + ih * sXH + iw * sXW\n                x_vals = tl.load(x_ptrs, mask=m, other=0.0).to(tl.float32)\n                w_val = tl.load(w_ptr + (oc * sWO + ic * sWI + kh * sWKH + kw * sWKW)).to(tl.float32)\n                acc += x_vals * w_val\n\n    acc += b\n\n    # BatchNorm2d (eval): y = (x - mean) / sqrt(var + eps) * gamma + beta\n    mean = tl.load(mean_ptr + oc).to(tl.float32)\n    var = tl.load(var_ptr + oc).to(tl.float32)\n    gamma = tl.load(gamma_ptr + oc).to(tl.float32)\n    beta = tl.load(beta_ptr + oc).to(tl.float32)\n    inv_std = 1.0 / tl.sqrt(var + EPS)\n    bn_out = (acc - mean) * inv_std\n    bn_out = bn_out * gamma + beta\n\n    # Softmax along last dim (width)\n    # Stability: subtract row max\n    neg_large = tl.full((BLOCK_W,), -1.0e30, dtype=tl.float32)\n    masked_vals = tl.where(mask_w, bn_out, neg_large)\n    row_max = tl.max(masked_vals, axis=0)\n    shifted = masked_vals - row_max\n    ex = tl.exp(shifted)\n    ex = tl.where(mask_w, ex, 0.0)\n    denom = tl.sum(ex, axis=0)\n    sm = ex / denom\n\n    y_ptrs = y_ptr + n * sYN + oc * sYC + h * sYH + offs_w * sYW\n    tl.store(y_ptrs, sm.to(y_ptr.dtype.element_ty), mask=mask_w)\n\n\ndef conv3_bn_softmax(x, w, b, gamma, beta, running_mean, running_var, eps=1e-5, block_w=512):\n    # x: [N,Cin,H,W], w: [Cout,Cin,3,3]\n    assert x.is_cuda and w.is_cuda\n    N, Cin, H, W = x.shape\n    Cout = w.shape[0]\n    y = torch.empty((N, Cout, H, W), device=x.device, dtype=x.dtype)\n\n    grid = (triton.cdiv(W, block_w), N * Cout * H)\n    _conv3_bn_sm_row_kernel[grid](\n        x, w, b, gamma, beta, running_mean, running_var, y,\n        N, Cin, H, W, Cout,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        w.stride(0), w.stride(1), w.stride(2), w.stride(3),\n        y.stride(0), y.stride(1), y.stride(2), y.stride(3),\n        EPS=eps, BLOCK_W=block_w\n    )\n    return y\n\n\n# ============================\n# 1x1 Conv (no BN / no Softmax)\n# ============================\n@triton.jit\ndef _conv1x1_row_kernel(\n    x_ptr, w_ptr, b_ptr, y_ptr,\n    N, Cin, H, W, Cout,\n    sXN, sXC, sXH, sXW,\n    sWO, sWI, sWKH, sWKW,  # still pass 4D strides for consistency (KH=KW=1)\n    sYN, sYC, sYH, sYW,\n    BLOCK_W: tl.constexpr,\n):\n    pid_w = tl.program_id(axis=0)\n    pid_r = tl.program_id(axis=1)\n\n    row_size = Cout * H\n    n = pid_r // row_size\n    rem = pid_r % row_size\n    oc = rem // H\n    h = rem % H\n\n    w_start = pid_w * BLOCK_W\n    offs_w = w_start + tl.arange(0, BLOCK_W)\n    mask_w = offs_w < W\n\n    acc = tl.zeros((BLOCK_W,), dtype=tl.float32)\n    b = tl.load(b_ptr + oc).to(tl.float32)\n\n    for ic in tl.range(0, Cin):\n        x_ptrs = x_ptr + n * sXN + ic * sXC + h * sXH + offs_w * sXW\n        x_vals = tl.load(x_ptrs, mask=mask_w, other=0.0).to(tl.float32)\n        w_val = tl.load(w_ptr + (oc * sWO + ic * sWI)).to(tl.float32)\n        acc += x_vals * w_val\n\n    acc += b\n    y_ptrs = y_ptr + n * sYN + oc * sYC + h * sYH + offs_w * sYW\n    tl.store(y_ptrs, acc.to(y_ptr.dtype.element_ty), mask=mask_w)\n\n\ndef conv1x1(x, w, b, block_w=512):\n    # x: [N,Cin,H,W], w: [Cout,Cin,1,1]\n    N, Cin, H, W = x.shape\n    Cout = w.shape[0]\n    y = torch.empty((N, Cout, H, W), device=x.device, dtype=x.dtype)\n    grid = (triton.cdiv(W, block_w), N * Cout * H)\n    _conv1x1_row_kernel[grid](\n        x, w, b, y,\n        N, Cin, H, W, Cout,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        w.stride(0), w.stride(1), w.stride(2), w.stride(3),\n        y.stride(0), y.stride(1), y.stride(2), y.stride(3),\n        BLOCK_W=block_w\n    )\n    return y\n\n\n# ============================\n# MaxPool2d kernel_size=2, stride=2\n# ============================\n@triton.jit\ndef _maxpool2x2_row_kernel(\n    x_ptr, y_ptr,\n    N, C, H_in, W_in, H_out, W_out,\n    sXN, sXC, sXH, sXW,\n    sYN, sYC, sYH, sYW,\n    BLOCK_W: tl.constexpr,\n):\n    pid_w = tl.program_id(axis=0)\n    pid_r = tl.program_id(axis=1)\n\n    row_size = C * H_out\n    n = pid_r // row_size\n    rem = pid_r % row_size\n    c = rem // H_out\n    ho = rem % H_out\n\n    w_start = pid_w * BLOCK_W\n    offs_wo = w_start + tl.arange(0, BLOCK_W)\n    mask_w = offs_wo < W_out\n\n    hi0 = 2 * ho\n    wi0 = 2 * offs_wo\n\n    x00_ptrs = x_ptr + n * sXN + c * sXC + (hi0 + 0) * sXH + (wi0 + 0) * sXW\n    x01_ptrs = x_ptr + n * sXN + c * sXC + (hi0 + 0) * sXH + (wi0 + 1) * sXW\n    x10_ptrs = x_ptr + n * sXN + c * sXC + (hi0 + 1) * sXH + (wi0 + 0) * sXW\n    x11_ptrs = x_ptr + n * sXN + c * sXC + (hi0 + 1) * sXH + (wi0 + 1) * sXW\n\n    # Inside bounds guards not necessary for even shapes here, but keep mask for safety\n    valid = mask_w  # shapes are exact halves in the test\n    x00 = tl.load(x00_ptrs, mask=valid, other=-1.0e30).to(tl.float32)\n    x01 = tl.load(x01_ptrs, mask=valid, other=-1.0e30).to(tl.float32)\n    x10 = tl.load(x10_ptrs, mask=valid, other=-1.0e30).to(tl.float32)\n    x11 = tl.load(x11_ptrs, mask=valid, other=-1.0e30).to(tl.float32)\n\n    m0 = tl.maximum(x00, x01)\n    m1 = tl.maximum(x10, x11)\n    m = tl.maximum(m0, m1)\n\n    y_ptrs = y_ptr + n * sYN + c * sYC + ho * sYH + offs_wo * sYW\n    tl.store(y_ptrs, m.to(y_ptr.dtype.element_ty), mask=mask_w)\n\n\ndef maxpool2x2(x, block_w=512):\n    N, C, H_in, W_in = x.shape\n    assert H_in % 2 == 0 and W_in % 2 == 0\n    H_out, W_out = H_in // 2, W_in // 2\n    y = torch.empty((N, C, H_out, W_out), device=x.device, dtype=x.dtype)\n    grid = (triton.cdiv(W_out, block_w), N * C * H_out)\n    _maxpool2x2_row_kernel[grid](\n        x, y,\n        N, C, H_in, W_in, H_out, W_out,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        y.stride(0), y.stride(1), y.stride(2), y.stride(3),\n        BLOCK_W=block_w\n    )\n    return y\n\n\n# ============================\n# ConvTranspose2d k=2, stride=2, padding=0\n# For this configuration, each output element depends on exactly one kernel offset:\n# kh = h_out % 2, kw = w_out % 2, and input location is (h_out//2, w_out//2)\n# Weight layout: [Cin, Cout, 2, 2] (PyTorch)\n# ============================\n@triton.jit\ndef _deconv2x2s2_row_kernel(\n    x_ptr, w_ptr, b_ptr, y_ptr,\n    N, Cin, H_in, W_in, Cout, H_out, W_out,\n    sXN, sXC, sXH, sXW,\n    sWO, sWI, sWKH, sWKW,  # For convtranspose: O==Cin, I==Cout here (using generic names)\n    sYN, sYC, sYH, sYW,\n    BLOCK_W: tl.constexpr,\n):\n    pid_w = tl.program_id(axis=0)\n    pid_r = tl.program_id(axis=1)\n\n    row_size = Cout * H_out\n    n = pid_r // row_size\n    rem = pid_r % row_size\n    oc = rem // H_out\n    ho = rem % H_out\n\n    w_start = pid_w * BLOCK_W\n    offs_wo = w_start + tl.arange(0, BLOCK_W)\n    mask_w = offs_wo < W_out\n\n    # map output positions to input positions (h_out -> h_in, w_out -> w_in)\n    hi = ho // 2\n    kh = ho % 2\n    wi = offs_wo // 2\n    kw_mask = (offs_wo % 2) == 1  # kw=1 if True else 0\n\n    acc = tl.zeros((BLOCK_W,), dtype=tl.float32)\n    b = tl.load(b_ptr + oc).to(tl.float32)\n    acc += b\n\n    for ic in tl.range(0, Cin):\n        x_ptrs = x_ptr + n * sXN + ic * sXC + hi * sXH + wi * sXW\n        x_vals = tl.load(x_ptrs, mask=mask_w, other=0.0).to(tl.float32)\n        # two possible kw per lane; pick via kw_mask\n        w0 = tl.load(w_ptr + (ic * sWO + oc * sWI + kh * sWKH + 0 * sWKW)).to(tl.float32)\n        w1 = tl.load(w_ptr + (ic * sWO + oc * sWI + kh * sWKH + 1 * sWKW)).to(tl.float32)\n        wv = tl.where(kw_mask, w1, w0)\n        acc += x_vals * wv\n\n    y_ptrs = y_ptr + n * sYN + oc * sYC + ho * sYH + offs_wo * sYW\n    tl.store(y_ptrs, acc.to(y_ptr.dtype.element_ty), mask=mask_w)\n\n\ndef deconv2x2s2(x, w, b, block_w=512):\n    # x: [N,Cin,H_in,W_in], w: [Cin,Cout,2,2]\n    N, Cin, H_in, W_in = x.shape\n    Cout = w.shape[1]\n    H_out, W_out = H_in * 2, W_in * 2\n    y = torch.empty((N, Cout, H_out, W_out), device=x.device, dtype=x.dtype)\n    grid = (triton.cdiv(W_out, block_w), N * Cout * H_out)\n    _deconv2x2s2_row_kernel[grid](\n        x, w, b, y,\n        N, Cin, H_in, W_in, Cout, H_out, W_out,\n        x.stride(0), x.stride(1), x.stride(2), x.stride(3),\n        w.stride(0), w.stride(1), w.stride(2), w.stride(3),\n        y.stride(0), y.stride(1), y.stride(2), y.stride(3),\n        BLOCK_W=block_w\n    )\n    return y\n\n\n# ============================\n# Channel-wise Concatenation via Copy with Channel Offset\n# ============================\n@triton.jit\ndef _cat_copy_row_kernel(\n    src_ptr, dst_ptr,\n    N, C, H, W, c_offset,\n    sSN, sSC, sSH, sSW,\n    sDN, sDC, sDH, sDW,\n    BLOCK_W: tl.constexpr,\n):\n    pid_w = tl.program_id(axis=0)\n    pid_r = tl.program_id(axis=1)\n\n    row_size = C * H\n    n = pid_r // row_size\n    rem = pid_r % row_size\n    c = rem // H\n    h = rem % H\n\n    w_start = pid_w * BLOCK_W\n    offs_w = w_start + tl.arange(0, BLOCK_W)\n    mask_w = offs_w < W\n\n    src_ptrs = src_ptr + n * sSN + c * sSC + h * sSH + offs_w * sSW\n    vals = tl.load(src_ptrs, mask=mask_w, other=0.0)\n    dst_ptrs = dst_ptr + n * sDN + (c + c_offset) * sDC + h * sDH + offs_w * sDW\n    tl.store(dst_ptrs, vals, mask=mask_w)\n\n\ndef cat_channels(x1, x2, block_w=512):\n    # Concatenate along channel dimension (N, C1 + C2, H, W) = cat([x1, x2], dim=1)\n    assert x1.shape[0] == x2.shape[0] and x1.shape[2:] == x2.shape[2:]\n    N, C1, H, W = x1.shape\n    C2 = x2.shape[1]\n    y = torch.empty((N, C1 + C2, H, W), device=x1.device, dtype=x1.dtype)\n\n    grid1 = (triton.cdiv(W, block_w), N * C1 * H)\n    _cat_copy_row_kernel[grid1](\n        x1, y,\n        N, C1, H, W, 0,\n        x1.stride(0), x1.stride(1), x1.stride(2), x1.stride(3),\n        y.stride(0), y.stride(1), y.stride(2), y.stride(3),\n        BLOCK_W=block_w\n    )\n    grid2 = (triton.cdiv(W, block_w), N * C2 * H)\n    _cat_copy_row_kernel[grid2](\n        x2, y,\n        N, C2, H, W, C1,\n        x2.stride(0), x2.stride(1), x2.stride(2), x2.stride(3),\n        y.stride(0), y.stride(1), y.stride(2), y.stride(3),\n        BLOCK_W=block_w\n    )\n    return y\n\n\n# ============================\n# Helpers to extract parameters from the provided state_dict\n# ============================\ndef _dc_params(state, prefix):\n    # DoubleConv params under f\"{prefix}.double_conv\"\n    p = f\"{prefix}.double_conv\"\n    conv1_w = state[f\"{p}.0.weight\"]\n    conv1_b = state[f\"{p}.0.bias\"]\n    bn1_w = state[f\"{p}.1.weight\"]\n    bn1_b = state[f\"{p}.1.bias\"]\n    bn1_mean = state[f\"{p}.1.running_mean\"]\n    bn1_var = state[f\"{p}.1.running_var\"]\n\n    conv2_w = state[f\"{p}.3.weight\"]\n    conv2_b = state[f\"{p}.3.bias\"]\n    bn2_w = state[f\"{p}.4.weight\"]\n    bn2_b = state[f\"{p}.4.bias\"]\n    bn2_mean = state[f\"{p}.4.running_mean\"]\n    bn2_var = state[f\"{p}.4.running_var\"]\n\n    return (conv1_w, conv1_b, bn1_w, bn1_b, bn1_mean, bn1_var,\n            conv2_w, conv2_b, bn2_w, bn2_b, bn2_mean, bn2_var)\n\n\ndef _up_params(state, name):\n    # ConvTranspose2d params: weight [Cin, Cout, 2, 2], bias [Cout]\n    w = state[f\"{name}.weight\"]\n    b = state[f\"{name}.bias\"]\n    return w, b\n\n\ndef _final_params(state):\n    w = state[\"final_conv.weight\"]\n    b = state[\"final_conv.bias\"]\n    return w, b\n\n\n# ============================\n# Orchestration: forward pass\n# ============================\ndef _double_conv_block(x, params, eps=1e-5):\n    (w1, b1, g1, be1, m1, v1,\n     w2, b2, g2, be2, m2, v2) = params\n    y = conv3_bn_softmax(x, w1, b1, g1, be1, m1, v1, eps=eps)\n    y = conv3_bn_softmax(y, w2, b2, g2, be2, m2, v2, eps=eps)\n    return y\n\n\ndef _forward_unet(x, state):\n    # Encoder\n    enc1 = _double_conv_block(x, _dc_params(state, \"encoder1\"))\n    p1 = maxpool2x2(enc1)\n    enc2 = _double_conv_block(p1, _dc_params(state, \"encoder2\"))\n    p2 = maxpool2x2(enc2)\n    enc3 = _double_conv_block(p2, _dc_params(state, \"encoder3\"))\n    p3 = maxpool2x2(enc3)\n    enc4 = _double_conv_block(p3, _dc_params(state, \"encoder4\"))\n    p4 = maxpool2x2(enc4)\n\n    # Bottleneck\n    bottleneck = _double_conv_block(p4, _dc_params(state, \"bottleneck\"))\n\n    # Decoder\n    up4_w, up4_b = _up_params(state, \"upconv4\")\n    dec4 = deconv2x2s2(bottleneck, up4_w, up4_b)\n    dec4 = cat_channels(dec4, enc4)\n    dec4 = _double_conv_block(dec4, _dc_params(state, \"decoder4\"))\n\n    up3_w, up3_b = _up_params(state, \"upconv3\")\n    dec3 = deconv2x2s2(dec4, up3_w, up3_b)\n    dec3 = cat_channels(dec3, enc3)\n    dec3 = _double_conv_block(dec3, _dc_params(state, \"decoder3\"))\n\n    up2_w, up2_b = _up_params(state, \"upconv2\")\n    dec2 = deconv2x2s2(dec3, up2_w, up2_b)\n    dec2 = cat_channels(dec2, enc2)\n    dec2 = _double_conv_block(dec2, _dc_params(state, \"decoder2\"))\n\n    up1_w, up1_b = _up_params(state, \"upconv1\")\n    dec1 = deconv2x2s2(dec2, up1_w, up1_b)\n    dec1 = cat_channels(dec1, enc1)\n    dec1 = _double_conv_block(dec1, _dc_params(state, \"decoder1\"))\n\n    # Final 1x1 conv to out_channels\n    fw, fb = _final_params(state)\n    out = conv1x1(dec1, fw, fb)\n    return out\n\n\n# ============================\n# Public API\n# ============================\ndef kernel_function(x, state_dict):\n    \"\"\"\n    Execute the full forward pass of the provided U-Net-like model using Triton kernels.\n\n    Args:\n      x: Input tensor of shape [N, C_in, H, W], on CUDA, dtype bf16 preferred.\n      state_dict: A PyTorch state dict (tensors on same device/dtype) containing all model weights/buffers.\n\n    Returns:\n      Output tensor [N, out_channels, H, W], same device/dtype as x.\n    \"\"\"\n    # Validation\n    if not isinstance(x, torch.Tensor):\n        raise TypeError(\"x must be a torch.Tensor\")\n    if not x.is_cuda:\n        raise ValueError(\"x must be on CUDA device\")\n    if not x.is_contiguous():\n        x = x.contiguous()\n    # Basic dtype/device checks on state_dict\n    if not isinstance(state_dict, dict):\n        raise TypeError(\"state_dict must be a dict of tensors\")\n    dev = x.device\n    for k, v in state_dict.items():\n        if isinstance(v, torch.Tensor):\n            if v.device != dev:\n                raise ValueError(f\"State tensor {k} is on {v.device}, expected {dev}\")\n            # ensure contiguous\n            if not v.is_contiguous():\n                state_dict[k] = v.contiguous()\n\n    # Run the network\n    with torch.no_grad():\n        y = _forward_unet(x, state_dict)\n    return y",
      "stdout": "",
      "stderr": ""
    }
  ]
}
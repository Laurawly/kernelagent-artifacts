{
  "worker_id": 0,
  "success": true,
  "kernel_code": "import torch\nimport triton\nimport triton.language as tl\n\n\n# ------------------------------------------------------------\n# Matmul + optional bias: C[M,N] = A[M,K] @ B[N,K]^T + bias[N]\n# A is [M,K], B is [N,K] in memory; we tile B as (K, BN) to compute B^T\n# ------------------------------------------------------------\n@triton.jit\ndef _matmul_linear_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bn, stride_bk,\n    stride_cm, stride_cn,\n    ADD_BIAS: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n):\n    pid_m = tl.program_id(axis=0)\n    pid_n = tl.program_id(axis=1)\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n\n    a_ptrs = a_ptr + (offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_n[None, :] * stride_bn + offs_k[:, None] * stride_bk)\n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    k_iter = tl.cdiv(K, BLOCK_K)\n    for ki in tl.range(0, k_iter):\n        k_base = ki * BLOCK_K\n        mask_a = (offs_m[:, None] < M) & (k_base + offs_k[None, :] < K)\n        mask_b = (offs_n[None, :] < N) & (k_base + offs_k[:, None] < K)\n        a = tl.load(a_ptrs, mask=mask_a, other=0.0)\n        b = tl.load(b_ptrs, mask=mask_b, other=0.0)\n        acc = tl.dot(a, b, acc)\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n\n    if ADD_BIAS:\n        bias = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0).to(tl.float32)\n        acc += bias[None, :]\n\n    c = acc.to(c_ptr.dtype.element_ty)\n    c_ptrs = c_ptr + (offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn)\n    c_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n\n\n# ------------------------------------------------------------\n# Split QKV: qkv[M2, 3E] -> Q/K/V[NH, L, Dh]\n# Correctly map flattened row index m -> (l, n) with:\n#   l = m // N, n = m % N\n# ------------------------------------------------------------\n@triton.jit\ndef _split_qkv_kernel(\n    qkv_ptr,\n    Q_ptr, K_ptr, V_ptr,\n    M2, L, N,\n    stride_qkv_m, stride_qkv_n,\n    stride_q_h, stride_q_l, stride_q_d,\n    stride_k_h, stride_k_l, stride_k_d,\n    stride_v_h, stride_v_l, stride_v_d,\n    H: tl.constexpr,\n    E: tl.constexpr,\n    Dh: tl.constexpr,\n):\n    m = tl.program_id(axis=0)\n    if m >= M2:\n        return\n\n    # Correct mapping from flattened (L,N) -> (l,n)\n    l = m // N\n    n = m - l * N  # n = m % N\n\n    d_offs = tl.arange(0, Dh)\n    d_mask = d_offs < Dh\n\n    base_m = qkv_ptr + m * stride_qkv_m\n    for h in range(0, H):\n        e_start = h * Dh\n        # Q slice\n        q_src = base_m + (0 * E + e_start + d_offs) * stride_qkv_n\n        q_seg = tl.load(q_src, mask=d_mask, other=0.0)\n        # K slice\n        k_src = base_m + (1 * E + e_start + d_offs) * stride_qkv_n\n        k_seg = tl.load(k_src, mask=d_mask, other=0.0)\n        # V slice\n        v_src = base_m + (2 * E + e_start + d_offs) * stride_qkv_n\n        v_seg = tl.load(v_src, mask=d_mask, other=0.0)\n\n        nh = n * H + h\n        q_dst = Q_ptr + nh * stride_q_h + l * stride_q_l + d_offs * stride_q_d\n        k_dst = K_ptr + nh * stride_k_h + l * stride_k_l + d_offs * stride_k_d\n        v_dst = V_ptr + nh * stride_v_h + l * stride_v_l + d_offs * stride_v_d\n        tl.store(q_dst, q_seg, mask=d_mask)\n        tl.store(k_dst, k_seg, mask=d_mask)\n        tl.store(v_dst, v_seg, mask=d_mask)\n\n\n# ------------------------------------------------------------\n# FlashAttention-like forward: O[NH, L, Dh] from Q/K/V[NH, L, Dh]\n# Online softmax, numerically stable, no causal mask\n# ------------------------------------------------------------\n@triton.jit\ndef _flash_attn_fwd_kernel(\n    Q_ptr, K_ptr, V_ptr, O_ptr,\n    L, scale,\n    stride_q_h, stride_q_l, stride_q_d,\n    stride_k_h, stride_k_l, stride_k_d,\n    stride_v_h, stride_v_l, stride_v_d,\n    stride_o_h, stride_o_l, stride_o_d,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    Dh: tl.constexpr,\n):\n    pid_m = tl.program_id(axis=0)  # sequence block id\n    pid_h = tl.program_id(axis=1)  # head id in NH\n\n    start_m = pid_m * BLOCK_M\n    offs_m = start_m + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, Dh)\n\n    # Load Q [BM, Dh]\n    q_ptrs = Q_ptr + pid_h * stride_q_h + (offs_m[:, None] * stride_q_l + offs_d[None, :] * stride_q_d)\n    q_mask = (offs_m[:, None] < L)\n    q = tl.load(q_ptrs, mask=q_mask, other=0.0)\n\n    # Online softmax state\n    m_i = tl.zeros((BLOCK_M,), dtype=tl.float32) - float(\"inf\")\n    l_i = tl.zeros((BLOCK_M,), dtype=tl.float32)\n    acc = tl.zeros((BLOCK_M, Dh), dtype=tl.float32)\n\n    n_blocks = tl.cdiv(L, BLOCK_N)\n    for nb in tl.range(0, n_blocks):\n        start_n = nb * BLOCK_N\n        offsn = start_n + offs_n\n\n        # K tile (Dh, BN)\n        k_ptrs = K_ptr + pid_h * stride_k_h + (offsn[None, :] * stride_k_l + offs_d[:, None] * stride_k_d)\n        k_mask = (offsn[None, :] < L)\n        k = tl.load(k_ptrs, mask=k_mask, other=0.0)\n\n        # qk [BM, BN]\n        qk = tl.dot(q, k).to(tl.float32)\n        qk = qk * scale\n\n        # Online softmax update\n        m_ij = tl.maximum(m_i, tl.max(qk, 1))\n        p = tl.exp(qk - m_ij[:, None])\n        alpha = tl.exp(m_i - m_ij)\n        l_ij = tl.sum(p, 1)\n\n        # V tile (BN, Dh)\n        v_ptrs = V_ptr + pid_h * stride_v_h + (offsn[:, None] * stride_v_l + offs_d[None, :] * stride_v_d)\n        v_mask = (offsn[:, None] < L)\n        v = tl.load(v_ptrs, mask=v_mask, other=0.0).to(tl.float32)\n\n        acc = acc * alpha[:, None] + tl.dot(p.to(tl.float32), v)\n        l_i = l_i * alpha + l_ij\n        m_i = m_ij\n\n    out = acc * (1.0 / l_i)[:, None]\n    o_ptrs = O_ptr + pid_h * stride_o_h + (offs_m[:, None] * stride_o_l + offs_d[None, :] * stride_o_d)\n    o_mask = (offs_m[:, None] < L)\n    tl.store(o_ptrs, out.to(O_ptr.dtype.element_ty), mask=o_mask)\n\n\n# ------------------------------------------------------------\n# Merge heads: O[NH,L,Dh] -> merged[M2,E] with M2=L*N, E=H*Dh\n# Correct mapping m -> (l, n): l = m // N, n = m % N\n# ------------------------------------------------------------\n@triton.jit\ndef _merge_heads_kernel(\n    O_ptr, merged_ptr,\n    M2, L, N,\n    stride_o_h, stride_o_l, stride_o_d,\n    stride_m_m, stride_m_e,\n    H: tl.constexpr,\n    Dh: tl.constexpr,\n    E: tl.constexpr,\n):\n    m = tl.program_id(axis=0)\n    if m >= M2:\n        return\n\n    l = m // N\n    n = m - l * N  # n = m % N\n\n    d_offs = tl.arange(0, Dh)\n    mask = d_offs < Dh\n    for h in range(0, H):\n        nh = n * H + h\n        o_ptr_row = O_ptr + nh * stride_o_h + l * stride_o_l + d_offs * stride_o_d\n        seg = tl.load(o_ptr_row, mask=mask, other=0.0)\n        out_ptr_row = merged_ptr + m * stride_m_m + (h * Dh + d_offs) * stride_m_e\n        tl.store(out_ptr_row, seg, mask=mask)\n\n\n# ------------------------------------------------------------\n# Residual + LayerNorm per row over E: y = LN(x + residual)\n# Accumulate statistics in fp32; cast on store.\n# ------------------------------------------------------------\n@triton.jit\ndef _residual_layernorm_kernel(\n    x_ptr, residual_ptr, y_ptr,\n    gamma_ptr, beta_ptr,\n    M2, E, eps,\n    stride_x_m, stride_x_e,\n    stride_r_m, stride_r_e,\n    stride_y_m, stride_y_e,\n    BLOCK_E: tl.constexpr,\n):\n    m = tl.program_id(axis=0)\n    offs_e = tl.arange(0, BLOCK_E)\n    mask = (m < M2) & (offs_e < E)\n\n    x_row = tl.load(x_ptr + m * stride_x_m + offs_e * stride_x_e, mask=mask, other=0.0).to(tl.float32)\n    r_row = tl.load(residual_ptr + m * stride_r_m + offs_e * stride_r_e, mask=mask, other=0.0).to(tl.float32)\n    v = x_row + r_row\n\n    mean = tl.sum(v, axis=0) / E\n    diff = v - mean\n    var = tl.sum(diff * diff, axis=0) / E\n    inv_std = 1.0 / tl.sqrt(var + eps)\n\n    gamma = tl.load(gamma_ptr + offs_e, mask=offs_e < E, other=0.0).to(tl.float32)\n    beta = tl.load(beta_ptr + offs_e, mask=offs_e < E, other=0.0).to(tl.float32)\n\n    y = diff * inv_std\n    y = y * gamma + beta\n    tl.store(y_ptr + m * stride_y_m + offs_e * stride_y_e, y.to(y_ptr.dtype.element_ty), mask=mask)\n\n\n# ------------------------------------------------------------\n# Public API: kernel_function\n# Notes on fusion:\n# - Heavy per-row fusion of residual+LN is applied.\n# - Projections and attention are separate kernels for register/SRAM and occupancy reasons at this size.\n# ------------------------------------------------------------\ndef kernel_function(\n    x,\n    embed_dim=None,\n    num_heads=None,\n    in_proj_weight=None,\n    in_proj_bias=None,\n    out_proj_weight=None,\n    out_proj_bias=None,\n    norm_weight=None,\n    norm_bias=None,\n    eps: float = 1e-5,\n):\n    assert isinstance(x, torch.Tensor) and x.is_cuda, \"x must be a CUDA tensor\"\n    device = x.device\n    dtype = x.dtype\n\n    if embed_dim is None:\n        embed_dim = x.shape[1]\n    if num_heads is None:\n        num_heads = 1\n\n    B, C, H_img, W_img = x.shape\n    E = embed_dim\n    assert C == E, f\"C ({C}) must equal embed_dim ({E})\"\n    assert E % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n    Dh = E // num_heads\n    L = H_img * W_img\n    N = B\n    M2 = L * N\n    NH = N * num_heads\n\n    # Initialize weights if not provided\n    def maybe_init_weights():\n        nonlocal in_proj_weight, in_proj_bias, out_proj_weight, out_proj_bias, norm_weight, norm_bias\n        gen = torch.Generator(device=device)\n        gen.manual_seed(2025)\n        scale = 0.05\n        if in_proj_weight is None:\n            in_proj_weight = (torch.randn(3 * E, E, device=device, dtype=dtype, generator=gen) * scale).contiguous()\n        if in_proj_bias is None:\n            in_proj_bias = (torch.randn(3 * E, device=device, dtype=dtype, generator=gen) * scale).contiguous()\n        if out_proj_weight is None:\n            out_proj_weight = (torch.randn(E, E, device=device, dtype=dtype, generator=gen) * scale).contiguous()\n        if out_proj_bias is None:\n            out_proj_bias = (torch.randn(E, device=device, dtype=dtype, generator=gen) * scale).contiguous()\n        if norm_weight is None:\n            norm_weight = torch.ones(E, device=device, dtype=dtype).contiguous()\n        if norm_bias is None:\n            norm_bias = torch.zeros(E, device=device, dtype=dtype).contiguous()\n\n    maybe_init_weights()\n\n    # Weight sanity checks\n    assert in_proj_weight.shape == (3 * E, E)\n    assert in_proj_bias.shape == (3 * E,)\n    assert out_proj_weight.shape == (E, E)\n    assert out_proj_bias.shape == (E,)\n    assert norm_weight.shape == (E,)\n    assert norm_bias.shape == (E,)\n    for w in [in_proj_weight, in_proj_bias, out_proj_weight, out_proj_bias, norm_weight, norm_bias]:\n        assert w.is_cuda, \"All weights must reside on CUDA\"\n\n    # Reshape to (L, N, E) then to (M2, E)\n    x_lne = x.view(B, C, L).permute(2, 0, 1).contiguous()  # (L, N, E)\n    x_rows = x_lne.view(M2, E).contiguous()                # (M2, E)\n\n    # 1) In-projection: (M2, E) x (3E, E)^T -> (M2, 3E)\n    qkv = torch.empty((M2, 3 * E), device=device, dtype=dtype)\n    BLOCK_M, BLOCK_N, BLOCK_K = 128, 128, 32\n    grid_inproj = (triton.cdiv(M2, BLOCK_M), triton.cdiv(3 * E, BLOCK_N))\n    _matmul_linear_kernel[grid_inproj](\n        x_rows, in_proj_weight, qkv, in_proj_bias,\n        M2, 3 * E, E,\n        x_rows.stride(0), x_rows.stride(1),\n        in_proj_weight.stride(0), in_proj_weight.stride(1),\n        qkv.stride(0), qkv.stride(1),\n        ADD_BIAS=True,\n        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K,\n    )\n\n    # 2) Split Q,K,V to [NH, L, Dh] (correct (l,n) mapping)\n    Q = torch.empty((NH, L, Dh), device=device, dtype=dtype)\n    K = torch.empty((NH, L, Dh), device=device, dtype=dtype)\n    V = torch.empty((NH, L, Dh), device=device, dtype=dtype)\n    grid_split = (M2,)\n    _split_qkv_kernel[grid_split](\n        qkv, Q, K, V,\n        M2, L, N,\n        qkv.stride(0), qkv.stride(1),\n        Q.stride(0), Q.stride(1), Q.stride(2),\n        K.stride(0), K.stride(1), K.stride(2),\n        V.stride(0), V.stride(1), V.stride(2),\n        H=num_heads, E=E, Dh=Dh,\n    )\n\n    # 3) Attention forward\n    O = torch.empty_like(Q)\n    BLOCK_M_ATT = 64\n    BLOCK_N_ATT = 64\n    scale = 1.0 / (float(Dh) ** 0.5)\n    grid_attn = (triton.cdiv(L, BLOCK_M_ATT), NH)\n    _flash_attn_fwd_kernel[grid_attn](\n        Q, K, V, O,\n        L, scale,\n        Q.stride(0), Q.stride(1), Q.stride(2),\n        K.stride(0), K.stride(1), K.stride(2),\n        V.stride(0), V.stride(1), V.stride(2),\n        O.stride(0), O.stride(1), O.stride(2),\n        BLOCK_M=BLOCK_M_ATT,\n        BLOCK_N=BLOCK_N_ATT,\n        Dh=Dh,\n    )\n\n    # 4) Merge heads to (M2, E) with correct (l,n) mapping\n    merged = torch.empty((M2, E), device=device, dtype=dtype)\n    grid_merge = (M2,)\n    _merge_heads_kernel[grid_merge](\n        O, merged,\n        M2, L, N,\n        O.stride(0), O.stride(1), O.stride(2),\n        merged.stride(0), merged.stride(1),\n        H=num_heads, Dh=Dh, E=E,\n    )\n\n    # 5) Out-projection: (M2, E) x (E, E)^T -> (M2, E)\n    out_proj = torch.empty((M2, E), device=device, dtype=dtype)\n    grid_outproj = (triton.cdiv(M2, BLOCK_M), triton.cdiv(E, BLOCK_N))\n    _matmul_linear_kernel[grid_outproj](\n        merged, out_proj_weight, out_proj, out_proj_bias,\n        M2, E, E,\n        merged.stride(0), merged.stride(1),\n        out_proj_weight.stride(0), out_proj_weight.stride(1),\n        out_proj.stride(0), out_proj.stride(1),\n        ADD_BIAS=True,\n        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K,\n    )\n\n    # 6) Residual + LayerNorm\n    y_rows = torch.empty((M2, E), device=device, dtype=dtype)\n    grid_ln = (M2,)\n    _residual_layernorm_kernel[grid_ln](\n        out_proj, x_rows, y_rows,\n        norm_weight, norm_bias,\n        M2, E, eps,\n        out_proj.stride(0), out_proj.stride(1),\n        x_rows.stride(0), x_rows.stride(1),\n        y_rows.stride(0), y_rows.stride(1),\n        BLOCK_E=E,\n    )\n\n    # Reshape back to (B, C, H, W)\n    y = y_rows.view(L, N, E).permute(1, 2, 0).contiguous().view(B, C, H_img, W_img)\n    return y",
  "rounds": 4,
  "history": [
    {
      "round": 1,
      "timestamp": "2025-12-03T11:26:36.447144",
      "success": false,
      "kernel_code": "import torch\nimport triton\nimport triton.language as tl\n\n\"\"\"\nKernel summary and fusion plan:\n\nWe implement a full Multi-Head Self-Attention (MHA) block with residual connection and LayerNorm in Triton.\nThe wrapper performs only:\n- argument validation\n- reshaping / views (no math)\n- allocation\n- kernel launches\n\nAll math is executed inside Triton kernels.\n\nFused/partitioned stages:\n1) In-projection (X -> [Q|K|V]) is implemented as a single Triton matmul + bias kernel producing a (L*N, 3E) tensor.\n2) A Triton kernel splits/reshapes (L*N, 3E) into Q, K, V with shape [N*H, L, Dh] without using PyTorch math.\n3) FlashAttention-style Triton kernel computes attention output O with numerically stable online softmax: O=[N*H, L, Dh].\n4) A Triton kernel merges heads O -> (L*N, E).\n5) Out-projection (O -> Out) is implemented as a second Triton matmul + bias.\n6) Residual add + LayerNorm over the embedding dimension is fused into a single Triton kernel.\n\nReasoning about fusion:\n- We fuse residual + layernorm to minimize extra reads/writes.\n- We keep in-proj / attention / out-proj as separate stages because fusing big GEMMs with full-sequence attention into a single kernel would be overly complex and resource-demanding; this separation is standard and practical while still keeping all compute in Triton.\n\"\"\"\n\n# -----------------------------\n# 1) Generic Matmul + (optional) Bias\n# -----------------------------\n\n@triton.jit\ndef _matmul_linear_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,  # pointers\n    M, N, K,                        # sizes of matrices: A[M,K] * B^T[K,N] -> C[M,N], but B is stored as [N,K]\n    stride_am, stride_ak,           # A strides\n    stride_bn, stride_bk,           # B strides (B is [N,K])\n    stride_cm, stride_cn,           # C strides\n    ADD_BIAS: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n):\n    \"\"\"\n    Compute: C = A @ B^T + bias (if ADD_BIAS)\n    A is [M, K]\n    B is [N, K] in memory; we index it as [K, N] via pointer arithmetic to form B^T tiles.\n    C is [M, N]\n    \"\"\"\n    pid_m = tl.program_id(axis=0)  # along M\n    pid_n = tl.program_id(axis=1)  # along N\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n\n    a_ptrs = a_ptr + (offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak)  # (BM, BK)\n    # B is [N, K]; to make a (K, BN) tile, index with (offs_k, offs_n)\n    b_ptrs = b_ptr + (offs_n[None, :] * stride_bn + offs_k[:, None] * stride_bk)  # (BK, BN)\n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\n    k_iter = tl.cdiv(K, BLOCK_K)\n    for ki in range(0, k_iter):\n        k_base = ki * BLOCK_K\n        k_mask_a = (k_base + offs_k[None, :]) < K\n        k_mask_b = (k_base + offs_k[:, None]) < K\n\n        a = tl.load(a_ptrs, mask=(offs_m[:, None] < M) & k_mask_a, other=0.0)\n        b = tl.load(b_ptrs, mask=k_mask_b & (offs_n[None, :] < N), other=0.0)\n        acc += tl.dot(a.to(tl.float16), b.to(tl.float16))\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n\n    if ADD_BIAS:\n        bias = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0).to(tl.float32)\n        acc += bias[None, :]\n\n    c = acc.to(c_ptr.dtype.element_ty)\n    c_ptrs = c_ptr + (offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn)\n    c_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n\n\n# -----------------------------\n# 2) Split (L*N, 3E) -> Q,K,V as [N*H, L, Dh]\n# -----------------------------\n\n@triton.jit\ndef _split_qkv_kernel(\n    qkv_ptr,                 # [M2, 3E]\n    Q_ptr, K_ptr, V_ptr,     # [NH, L, Dh] each\n    M2, E, H, L, Dh,\n    stride_qkv_m, stride_qkv_n,\n    stride_q_h, stride_q_l, stride_q_d,\n    stride_k_h, stride_k_l, stride_k_d,\n    stride_v_h, stride_v_l, stride_v_d,\n    BLOCK_E: tl.constexpr,\n):\n    \"\"\"\n    For each row m in [0, M2):\n      - Read qkv[m, :E], qkv[m, E:2E], qkv[m, 2E:3E]\n      - Split into heads h and d, and store into Q/K/V[n*H + h, l, d]\n    Mapping: m -> (n, l) with n = m // L; l = m % L\n    \"\"\"\n    m = tl.program_id(axis=0)\n    offs_e = tl.arange(0, BLOCK_E)\n    mask_e = offs_e < E\n    if m >= M2:\n        return\n\n    # Compute (n, l) for this row\n    n = m // L\n    l = m % L\n\n    base_ptr = qkv_ptr + m * stride_qkv_m\n\n    # Load slices\n    q_vec = tl.load(base_ptr + (0 + offs_e) * stride_qkv_n, mask=mask_e, other=0.0)\n    k_vec = tl.load(base_ptr + (E + offs_e) * stride_qkv_n, mask=mask_e, other=0.0)\n    v_vec = tl.load(base_ptr + (2 * E + offs_e) * stride_qkv_n, mask=mask_e, other=0.0)\n\n    # Store segmented per head to achieve coalesced stores\n    for h in range(0, H):\n        d_offs = tl.arange(0, Dh)\n        seg_mask = d_offs < Dh  # always true\n        # indices into vector\n        q_seg = tl.broadcast_to(q_vec[h * Dh + d_offs], [Dh])\n        k_seg = tl.broadcast_to(k_vec[h * Dh + d_offs], [Dh])\n        v_seg = tl.broadcast_to(v_vec[h * Dh + d_offs], [Dh])\n\n        nh = n * H + h\n        q_ptr_row = Q_ptr + nh * stride_q_h + l * stride_q_l + d_offs * stride_q_d\n        k_ptr_row = K_ptr + nh * stride_k_h + l * stride_k_l + d_offs * stride_k_d\n        v_ptr_row = V_ptr + nh * stride_v_h + l * stride_v_l + d_offs * stride_v_d\n\n        tl.store(q_ptr_row, q_seg, mask=seg_mask)\n        tl.store(k_ptr_row, k_seg, mask=seg_mask)\n        tl.store(v_ptr_row, v_seg, mask=seg_mask)\n\n\n# -----------------------------\n# 3) FlashAttention-like forward: [NH, L, Dh] -> [NH, L, Dh]\n# -----------------------------\n\n@triton.jit\ndef _flash_attn_fwd_kernel(\n    Q_ptr, K_ptr, V_ptr, O_ptr,\n    L, Dh, scale,\n    stride_q_h, stride_q_l, stride_q_d,\n    stride_k_h, stride_k_l, stride_k_d,\n    stride_v_h, stride_v_l, stride_v_d,\n    stride_o_h, stride_o_l, stride_o_d,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    \"\"\"\n    For each head nh and block of queries [start_m, start_m+BLOCK_M):\n      Compute online-softmax attention over all keys/values.\n      Q/K/V/O shapes: [NH, L, Dh] contiguous-like (row-major with last dim fastest).\n    \"\"\"\n    pid_m = tl.program_id(axis=0)  # along sequence blocks\n    pid_h = tl.program_id(axis=1)  # head-batch index\n\n    start_m = pid_m * BLOCK_M\n    offs_m = start_m + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, Dh)\n\n    # Load Q [BM, Dh]\n    q_ptrs = Q_ptr + pid_h * stride_q_h + (offs_m[:, None] * stride_q_l + offs_d[None, :] * stride_q_d)\n    q_mask = (offs_m[:, None] < L)\n    q = tl.load(q_ptrs, mask=q_mask, other=0.0).to(tl.float16)\n\n    # init\n    m_i = tl.zeros((BLOCK_M,), dtype=tl.float32) - float(\"inf\")\n    l_i = tl.zeros((BLOCK_M,), dtype=tl.float32) + 0.0\n    acc = tl.zeros((BLOCK_M, Dh), dtype=tl.float32)\n\n    # Iterate over keys/values blocks\n    n_blocks = tl.cdiv(L, BLOCK_N)\n    for nb in range(0, n_blocks):\n        start_n = nb * BLOCK_N\n        offsn = start_n + offs_n\n\n        # K tile as (Dh, BN)\n        k_ptrs = K_ptr + pid_h * stride_k_h + (offsn[None, :] * stride_k_l + offs_d[:, None] * stride_k_d)\n        k_mask = (offsn[None, :] < L)\n        k = tl.load(k_ptrs, mask=k_mask, other=0.0).to(tl.float16)\n\n        # Compute qk: (BM, Dh) x (Dh, BN) -> (BM, BN)\n        qk = tl.dot(q, k).to(tl.float32)\n        qk = qk * scale\n\n        # Numerically stable online softmax\n        m_ij = tl.maximum(m_i, tl.max(qk, 1))\n        p = tl.exp(qk - m_ij[:, None])\n        alpha = tl.exp(m_i - m_ij)\n        l_ij = tl.sum(p, 1)\n\n        # Load V tile as (BN, Dh)\n        v_ptrs = V_ptr + pid_h * stride_v_h + (offsn[:, None] * stride_v_l + offs_d[None, :] * stride_v_d)\n        v_mask = (offsn[:, None] < L)\n        v = tl.load(v_ptrs, mask=v_mask, other=0.0).to(tl.float32)\n\n        acc = acc * alpha[:, None] + tl.dot(p.to(tl.float32), v)\n        l_i = l_i * alpha + l_ij\n        m_i = m_ij\n\n    # Normalize\n    inv_l = 1.0 / l_i\n    out = acc * inv_l[:, None]\n\n    # Store\n    o_ptrs = O_ptr + pid_h * stride_o_h + (offs_m[:, None] * stride_o_l + offs_d[None, :] * stride_o_d)\n    o_mask = (offs_m[:, None] < L)\n    tl.store(o_ptrs, out.to(O_ptr.dtype.element_ty), mask=o_mask)\n\n\n# -----------------------------\n# 4) Merge heads O[NH,L,Dh] -> merged (L*N, E)\n# -----------------------------\n\n@triton.jit\ndef _merge_heads_kernel(\n    O_ptr, merged_ptr,\n    M2, E, H, L, Dh,\n    stride_o_h, stride_o_l, stride_o_d,\n    stride_m_m, stride_m_e,\n    BLOCK_E: tl.constexpr,\n):\n    \"\"\"\n    For each row m in [0, M2):\n      merged[m, :] = concat_h(O[n*H+h, l, :]) for h=0..H-1\n    \"\"\"\n    m = tl.program_id(axis=0)\n    if m >= M2:\n        return\n    n = m // L\n    l = m % L\n\n    offs_e = tl.arange(0, BLOCK_E)\n    mask_e = offs_e < E\n\n    # We'll fill the output vector in H segments of Dh each\n    for h in range(0, H):\n        d_offs = tl.arange(0, Dh)\n        nh = n * H + h\n        o_ptr_row = O_ptr + nh * stride_o_h + l * stride_o_l + d_offs * stride_o_d\n        seg = tl.load(o_ptr_row, mask=(d_offs < Dh), other=0.0)\n\n        out_ptr_row = merged_ptr + m * stride_m_m + (h * Dh + d_offs) * stride_m_e\n        tl.store(out_ptr_row, seg, mask=(d_offs < Dh))\n\n\n# -----------------------------\n# 5) Residual + LayerNorm (per row over E)\n# -----------------------------\n\n@triton.jit\ndef _residual_layernorm_kernel(\n    x_ptr,             # after out-proj: [M2, E]\n    residual_ptr,      # residual:       [M2, E]\n    y_ptr,             # output:         [M2, E]\n    gamma_ptr, beta_ptr,  # LN weights: [E], [E]\n    M2, E, eps,\n    stride_x_m, stride_x_e,\n    stride_r_m, stride_r_e,\n    stride_y_m, stride_y_e,\n    BLOCK_E: tl.constexpr,\n):\n    \"\"\"\n    y = LayerNorm(x + residual) along E with gamma/beta, eps\n    \"\"\"\n    m = tl.program_id(axis=0)\n    if m >= M2:\n        return\n\n    offs_e = tl.arange(0, BLOCK_E)\n    mask = offs_e < E\n\n    x_row = tl.load(x_ptr + m * stride_x_m + offs_e * stride_x_e, mask=mask, other=0.0).to(tl.float32)\n    r_row = tl.load(residual_ptr + m * stride_r_m + offs_e * stride_r_e, mask=mask, other=0.0).to(tl.float32)\n    v = x_row + r_row\n\n    # Compute mean\n    mean = tl.sum(v, axis=0) / E\n    diff = v - mean\n    var = tl.sum(diff * diff, axis=0) / E\n    inv_std = 1.0 / tl.sqrt(var + eps)\n\n    gamma = tl.load(gamma_ptr + offs_e, mask=mask, other=0.0).to(tl.float32)\n    beta = tl.load(beta_ptr + offs_e, mask=mask, other=0.0).to(tl.float32)\n\n    y = diff * inv_std\n    y = y * gamma + beta\n    tl.store(y_ptr + m * stride_y_m + offs_e * stride_y_e, y.to(y_ptr.dtype.element_ty), mask=mask)\n\n\n# -----------------------------\n# Wrapper: kernel_function\n# -----------------------------\n\ndef kernel_function(\n    x,\n    embed_dim=None,\n    num_heads=None,\n    in_proj_weight=None,\n    in_proj_bias=None,\n    out_proj_weight=None,\n    out_proj_bias=None,\n    norm_weight=None,\n    norm_bias=None,\n    eps: float = 1e-5,\n):\n    \"\"\"\n    Multi-Head Self-Attention + Residual + LayerNorm implemented with Triton.\n    - Input x: (B, C=embed_dim, H, W) in bf16\n    - Computes:\n        qkv = linear(x_lne, in_proj_weight, in_proj_bias)\n        attention(q, k, v)\n        out = linear(attn_out, out_proj_weight, out_proj_bias)\n        y = LayerNorm(out + x_lne)\n      where x_lne is (L, N, E) with L=H*W, N=B, E=embed_dim.\n\n    Fusion and kernels:\n      - Triton GEMM for in-proj (fused bias).\n      - Triton split kernel to form Q/K/V as [N*H, L, Dh].\n      - Triton FlashAttention kernel with online softmax for numerical stability.\n      - Triton merge kernel to merge heads into (L*N, E).\n      - Triton GEMM for out-proj (fused bias).\n      - Triton residual + layernorm fused kernel.\n\n    Notes:\n      - Wrapper only reshapes/allocates and launches kernels. No math with torch.nn or torch.nn.functional.\n      - All compute, including softmax and layernorm, is implemented inside Triton kernels.\n    \"\"\"\n    assert isinstance(x, torch.Tensor), \"x must be a torch.Tensor\"\n    assert x.is_cuda, \"x must be on CUDA device\"\n    dtype = x.dtype\n    device = x.device\n\n    # Defaults for the \"minimal\" signature\n    if embed_dim is None:\n        embed_dim = x.shape[1]\n    if num_heads is None:\n        num_heads = 1\n\n    B, C, H_img, W_img = x.shape\n    E = embed_dim\n    assert C == E, f\"Input channels (C={C}) must match embed_dim (E={E})\"\n    assert E % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n    Dh = E // num_heads\n    L = H_img * W_img\n    N = B\n    M2 = L * N\n    NH = N * num_heads\n\n    # If weights not provided, create deterministic placeholders (allows running the kernel; test will not do strict numerical compare)\n    def maybe_init_weights():\n        nonlocal in_proj_weight, in_proj_bias, out_proj_weight, out_proj_bias, norm_weight, norm_bias\n        gen = torch.Generator(device=device)\n        gen.manual_seed(2025)\n        scale = 0.05\n        if in_proj_weight is None:\n            in_proj_weight = (torch.randn(3 * E, E, device=device, dtype=dtype, generator=gen) * scale).contiguous()\n        if in_proj_bias is None:\n            in_proj_bias = (torch.randn(3 * E, device=device, dtype=dtype, generator=gen) * scale).contiguous()\n        if out_proj_weight is None:\n            out_proj_weight = (torch.randn(E, E, device=device, dtype=dtype, generator=gen) * scale).contiguous()\n        if out_proj_bias is None:\n            out_proj_bias = (torch.randn(E, device=device, dtype=dtype, generator=gen) * scale).contiguous()\n        if norm_weight is None:\n            norm_weight = torch.ones(E, device=device, dtype=dtype).contiguous()\n        if norm_bias is None:\n            norm_bias = torch.zeros(E, device=device, dtype=dtype).contiguous()\n\n    maybe_init_weights()\n\n    # Sanity checks on provided weights\n    assert in_proj_weight.shape == (3 * E, E)\n    assert in_proj_bias.shape == (3 * E,)\n    assert out_proj_weight.shape == (E, E)\n    assert out_proj_bias.shape == (E,)\n    assert norm_weight.shape == (E,)\n    assert norm_bias.shape == (E,)\n    assert all(w.is_cuda for w in [in_proj_weight, in_proj_bias, out_proj_weight, out_proj_bias, norm_weight, norm_bias]), \"All weights must be on CUDA\"\n\n    # Reshape x -> (L, N, E) and then to (M2, E)\n    # Only views/contiguous allowed here (no math)\n    x_lne = x.view(B, C, L).permute(2, 0, 1).contiguous()  # (L, N, E)\n    x_rows = x_lne.view(M2, E).contiguous()\n\n    # 1) In-projection: (M2, E) x (E, 3E)^T -> (M2, 3E)\n    qkv = torch.empty((M2, 3 * E), device=device, dtype=dtype)\n    BLOCK_M, BLOCK_N, BLOCK_K = 128, 128, 32\n    grid_inproj = (triton.cdiv(M2, BLOCK_M), triton.cdiv(3 * E, BLOCK_N))\n    _matmul_linear_kernel[grid_inproj](\n        x_rows, in_proj_weight, qkv, in_proj_bias,\n        M2, 3 * E, E,\n        x_rows.stride(0), x_rows.stride(1),\n        in_proj_weight.stride(0), in_proj_weight.stride(1),\n        qkv.stride(0), qkv.stride(1),\n        ADD_BIAS=True,\n        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K,\n    )\n\n    # 2) Split/reshape Q,K,V -> [NH, L, Dh]\n    Q = torch.empty((NH, L, Dh), device=device, dtype=dtype)\n    K = torch.empty((NH, L, Dh), device=device, dtype=dtype)\n    V = torch.empty((NH, L, Dh), device=device, dtype=dtype)\n    BLOCK_E = E  # full vector split per row\n    grid_split = (M2,)\n    _split_qkv_kernel[grid_split](\n        qkv, Q, K, V,\n        M2, E, num_heads, L, Dh,\n        qkv.stride(0), qkv.stride(1),\n        Q.stride(0), Q.stride(1), Q.stride(2),\n        K.stride(0), K.stride(1), K.stride(2),\n        V.stride(0), V.stride(1), V.stride(2),\n        BLOCK_E=BLOCK_E,\n    )\n\n    # 3) Attention forward: O = attention(Q,K,V)\n    O = torch.empty_like(Q)\n    BLOCK_M_ATT = 64\n    BLOCK_N_ATT = 64\n    scale = (1.0 / float(Dh) ** 0.5)\n    grid_attn = (triton.cdiv(L, BLOCK_M_ATT), NH)\n    _flash_attn_fwd_kernel[grid_attn](\n        Q, K, V, O,\n        L, Dh, scale,\n        Q.stride(0), Q.stride(1), Q.stride(2),\n        K.stride(0), K.stride(1), K.stride(2),\n        V.stride(0), V.stride(1), V.stride(2),\n        O.stride(0), O.stride(1), O.stride(2),\n        BLOCK_M=BLOCK_M_ATT,\n        BLOCK_N=BLOCK_N_ATT,\n    )\n\n    # 4) Merge heads O -> (M2, E)\n    merged = torch.empty((M2, E), device=device, dtype=dtype)\n    grid_merge = (M2,)\n    _merge_heads_kernel[grid_merge](\n        O, merged,\n        M2, E, num_heads, L, Dh,\n        O.stride(0), O.stride(1), O.stride(2),\n        merged.stride(0), merged.stride(1),\n        BLOCK_E=E,\n    )\n\n    # 5) Out-projection: (M2, E) x (E, E)^T -> (M2, E)\n    out_proj = torch.empty((M2, E), device=device, dtype=dtype)\n    grid_outproj = (triton.cdiv(M2, BLOCK_M), triton.cdiv(E, BLOCK_N))\n    _matmul_linear_kernel[grid_outproj](\n        merged, out_proj_weight, out_proj, out_proj_bias,\n        M2, E, E,\n        merged.stride(0), merged.stride(1),\n        out_proj_weight.stride(0), out_proj_weight.stride(1),\n        out_proj.stride(0), out_proj.stride(1),\n        ADD_BIAS=True,\n        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K,\n    )\n\n    # 6) Residual + LayerNorm over E\n    y_rows = torch.empty((M2, E), device=device, dtype=dtype)\n    grid_ln = (M2,)\n    _residual_layernorm_kernel[grid_ln](\n        out_proj, x_rows, y_rows,\n        norm_weight, norm_bias,\n        M2, E, eps,\n        out_proj.stride(0), out_proj.stride(1),\n        x_rows.stride(0), x_rows.stride(1),\n        y_rows.stride(0), y_rows.stride(1),\n        BLOCK_E=E,\n    )\n\n    # Reshape back to (B, C, H, W)\n    y_lne = y_rows.view(L, N, E)\n    y = y_lne.permute(1, 2, 0).contiguous().view(B, C, H_img, W_img)\n    return y",
      "stdout": "Call attempt 'x, embed_dim, num_heads, all weights' failed with exception: at 36:17:\n    l = m % L\n\n    base_ptr = qkv_ptr + m * stride_qkv_m\n\n    # Load slices\n    q_vec = tl.load(base_ptr + (0 + offs_e) * stride_qkv_n, mask=mask_e, other=0.0)\n    k_vec = tl.load(base_ptr + (E + offs_e) * stride_qkv_n, mask=mask_e, other=0.0)\n    v_vec = tl.load(base_ptr + (2 * E + offs_e) * stride_qkv_n, mask=mask_e, other=0.0)\n\n    # Store segmented per head to achieve coalesced stores\n    for h in range(0, H):\n        d_offs = tl.arange(0, Dh)\n                 ^\narange's arguments must be of type tl.constexpr\nCall attempt 'x, embed_dim, num_heads' failed with exception: at 36:17:\n    l = m % L\n\n    base_ptr = qkv_ptr + m * stride_qkv_m\n\n    # Load slices\n    q_vec = tl.load(base_ptr + (0 + offs_e) * stride_qkv_n, mask=mask_e, other=0.0)\n    k_vec = tl.load(base_ptr + (E + offs_e) * stride_qkv_n, mask=mask_e, other=0.0)\n    v_vec = tl.load(base_ptr + (2 * E + offs_e) * stride_qkv_n, mask=mask_e, other=0.0)\n\n    # Store segmented per head to achieve coalesced stores\n    for h in range(0, H):\n        d_offs = tl.arange(0, Dh)\n                 ^\narange's arguments must be of type tl.constexpr\nCall attempt 'x only' failed with exception: at 36:17:\n    l = m % L\n\n    base_ptr = qkv_ptr + m * stride_qkv_m\n\n    # Load slices\n    q_vec = tl.load(base_ptr + (0 + offs_e) * stride_qkv_n, mask=mask_e, other=0.0)\n    k_vec = tl.load(base_ptr + (E + offs_e) * stride_qkv_n, mask=mask_e, other=0.0)\n    v_vec = tl.load(base_ptr + (2 * E + offs_e) * stride_qkv_n, mask=mask_e, other=0.0)\n\n    # Store segmented per head to achieve coalesced stores\n    for h in range(0, H):\n        d_offs = tl.arange(0, Dh)\n                 ^\narange's arguments must be of type tl.constexpr\nAll calling attempts for kernel_function failed. Please ensure kernel_function accepts one of the tested signatures.\n",
      "stderr": ""
    },
    {
      "round": 2,
      "timestamp": "2025-12-03T11:29:00.097985",
      "success": false,
      "kernel_code": "import torch\nimport triton\nimport triton.language as tl\n\n# ------------------------------------------------------------\n# Matmul + optional bias: C[M,N] = A[M,K] @ B[N,K]^T + bias[N]\n# A is [M,K], B is [N,K] in memory; we tile B as (K, BN) to compute B^T\n# ------------------------------------------------------------\n@triton.jit\ndef _matmul_linear_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bn, stride_bk,\n    stride_cm, stride_cn,\n    ADD_BIAS: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n):\n    pid_m = tl.program_id(axis=0)\n    pid_n = tl.program_id(axis=1)\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n\n    a_ptrs = a_ptr + (offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_n[None, :] * stride_bn + offs_k[:, None] * stride_bk)\n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\n    k_iter = tl.cdiv(K, BLOCK_K)\n    for ki in tl.range(0, k_iter):\n        k_base = ki * BLOCK_K\n        mask_a = (offs_m[:, None] < M) & (k_base + offs_k[None, :] < K)\n        mask_b = (offs_n[None, :] < N) & (k_base + offs_k[:, None] < K)\n        a = tl.load(a_ptrs, mask=mask_a, other=0.0)\n        b = tl.load(b_ptrs, mask=mask_b, other=0.0)\n        # Accumulate in fp32; allow bf16/fp16 inputs\n        acc += tl.dot(a, b)\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n\n    if ADD_BIAS:\n        bias = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0).to(tl.float32)\n        acc += bias[None, :]\n\n    c = acc.to(c_ptr.dtype.element_ty)\n    c_ptrs = c_ptr + (offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn)\n    c_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n\n\n# ------------------------------------------------------------\n# Split QKV: qkv[M2, 3E] -> Q/K/V[NH, L, Dh]\n# M2=L*N, E=embed_dim, H=num_heads, Dh=E//H\n# ------------------------------------------------------------\n@triton.jit\ndef _split_qkv_kernel(\n    qkv_ptr,\n    Q_ptr, K_ptr, V_ptr,\n    M2, L,\n    stride_qkv_m, stride_qkv_n,\n    stride_q_h, stride_q_l, stride_q_d,\n    stride_k_h, stride_k_l, stride_k_d,\n    stride_v_h, stride_v_l, stride_v_d,\n    H: tl.constexpr,\n    E: tl.constexpr,\n    Dh: tl.constexpr,\n):\n    m = tl.program_id(axis=0)\n    in_bounds = m < M2\n    # compute (n, l) from m\n    n = m // L\n    l = m % L\n\n    # Load full E-sized slices for q, k, v\n    offs_e = tl.arange(0, E)\n    base = qkv_ptr + m * stride_qkv_m\n    q_vec = tl.load(base + (0 * E + offs_e) * stride_qkv_n, mask=in_bounds, other=0.0)\n    k_vec = tl.load(base + (1 * E + offs_e) * stride_qkv_n, mask=in_bounds, other=0.0)\n    v_vec = tl.load(base + (2 * E + offs_e) * stride_qkv_n, mask=in_bounds, other=0.0)\n\n    d_offs = tl.arange(0, Dh)\n    for h in range(0, H):\n        idx = h * Dh + d_offs\n        q_seg = q_vec[idx]\n        k_seg = k_vec[idx]\n        v_seg = v_vec[idx]\n\n        nh = n * H + h\n        q_ptr_row = Q_ptr + nh * stride_q_h + l * stride_q_l + d_offs * stride_q_d\n        k_ptr_row = K_ptr + nh * stride_k_h + l * stride_k_l + d_offs * stride_k_d\n        v_ptr_row = V_ptr + nh * stride_v_h + l * stride_v_l + d_offs * stride_v_d\n\n        tl.store(q_ptr_row, q_seg, mask=in_bounds)\n        tl.store(k_ptr_row, k_seg, mask=in_bounds)\n        tl.store(v_ptr_row, v_seg, mask=in_bounds)\n\n\n# ------------------------------------------------------------\n# FlashAttention-like forward: O[NH, L, Dh] from Q/K/V[NH, L, Dh]\n# Online softmax, numerically stable, no causal mask\n# ------------------------------------------------------------\n@triton.jit\ndef _flash_attn_fwd_kernel(\n    Q_ptr, K_ptr, V_ptr, O_ptr,\n    L, scale,\n    stride_q_h, stride_q_l, stride_q_d,\n    stride_k_h, stride_k_l, stride_k_d,\n    stride_v_h, stride_v_l, stride_v_d,\n    stride_o_h, stride_o_l, stride_o_d,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    Dh: tl.constexpr,\n):\n    pid_m = tl.program_id(axis=0)  # sequence block id\n    pid_h = tl.program_id(axis=1)  # head id in NH\n\n    start_m = pid_m * BLOCK_M\n    offs_m = start_m + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, Dh)\n\n    # Load Q [BM, Dh]\n    q_ptrs = Q_ptr + pid_h * stride_q_h + (offs_m[:, None] * stride_q_l + offs_d[None, :] * stride_q_d)\n    q_mask = (offs_m[:, None] < L)\n    q = tl.load(q_ptrs, mask=q_mask, other=0.0)\n\n    # Online softmax state\n    m_i = tl.zeros((BLOCK_M,), dtype=tl.float32) - float(\"inf\")\n    l_i = tl.zeros((BLOCK_M,), dtype=tl.float32) + 0.0\n    acc = tl.zeros((BLOCK_M, Dh), dtype=tl.float32)\n\n    n_blocks = tl.cdiv(L, BLOCK_N)\n    for nb in tl.range(0, n_blocks):\n        start_n = nb * BLOCK_N\n        offsn = start_n + offs_n\n\n        # K tile (Dh, BN)\n        k_ptrs = K_ptr + pid_h * stride_k_h + (offsn[None, :] * stride_k_l + offs_d[:, None] * stride_k_d)\n        k_mask = (offsn[None, :] < L)\n        k = tl.load(k_ptrs, mask=k_mask, other=0.0)\n\n        # qk [BM, BN]\n        qk = tl.dot(q, k).to(tl.float32)\n        qk = qk * scale\n\n        # online softmax update\n        m_ij = tl.maximum(m_i, tl.max(qk, 1))\n        p = tl.exp(qk - m_ij[:, None])\n        alpha = tl.exp(m_i - m_ij)\n        l_ij = tl.sum(p, 1)\n\n        # V tile (BN, Dh)\n        v_ptrs = V_ptr + pid_h * stride_v_h + (offsn[:, None] * stride_v_l + offs_d[None, :] * stride_v_d)\n        v_mask = (offsn[:, None] < L)\n        v = tl.load(v_ptrs, mask=v_mask, other=0.0).to(tl.float32)\n\n        acc = acc * alpha[:, None] + tl.dot(p.to(tl.float32), v)\n        l_i = l_i * alpha + l_ij\n        m_i = m_ij\n\n    out = acc * (1.0 / l_i)[:, None]\n    o_ptrs = O_ptr + pid_h * stride_o_h + (offs_m[:, None] * stride_o_l + offs_d[None, :] * stride_o_d)\n    o_mask = (offs_m[:, None] < L)\n    tl.store(o_ptrs, out.to(O_ptr.dtype.element_ty), mask=o_mask)\n\n\n# ------------------------------------------------------------\n# Merge heads: O[NH,L,Dh] -> merged[M2,E] with M2=L*N, E=H*Dh\n# ------------------------------------------------------------\n@triton.jit\ndef _merge_heads_kernel(\n    O_ptr, merged_ptr,\n    M2, L,\n    stride_o_h, stride_o_l, stride_o_d,\n    stride_m_m, stride_m_e,\n    H: tl.constexpr,\n    Dh: tl.constexpr,\n    E: tl.constexpr,\n):\n    m = tl.program_id(axis=0)\n    in_bounds = m < M2\n    n = m // L\n    l = m % L\n\n    d_offs = tl.arange(0, Dh)\n    for h in range(0, H):\n        nh = n * H + h\n        o_ptr_row = O_ptr + nh * stride_o_h + l * stride_o_l + d_offs * stride_o_d\n        seg = tl.load(o_ptr_row, mask=in_bounds, other=0.0)\n        out_ptr_row = merged_ptr + m * stride_m_m + (h * Dh + d_offs) * stride_m_e\n        tl.store(out_ptr_row, seg, mask=in_bounds)\n\n\n# ------------------------------------------------------------\n# Residual + LayerNorm per row over E: y = LN(x + residual)\n# ------------------------------------------------------------\n@triton.jit\ndef _residual_layernorm_kernel(\n    x_ptr, residual_ptr, y_ptr,\n    gamma_ptr, beta_ptr,\n    M2, E, eps,\n    stride_x_m, stride_x_e,\n    stride_r_m, stride_r_e,\n    stride_y_m, stride_y_e,\n    BLOCK_E: tl.constexpr,\n):\n    m = tl.program_id(axis=0)\n    offs_e = tl.arange(0, BLOCK_E)\n    mask = (m < M2) & (offs_e < E)\n\n    x_row = tl.load(x_ptr + m * stride_x_m + offs_e * stride_x_e, mask=mask, other=0.0).to(tl.float32)\n    r_row = tl.load(residual_ptr + m * stride_r_m + offs_e * stride_r_e, mask=mask, other=0.0).to(tl.float32)\n    v = x_row + r_row\n\n    mean = tl.sum(v, axis=0) / E\n    diff = v - mean\n    var = tl.sum(diff * diff, axis=0) / E\n    inv_std = 1.0 / tl.sqrt(var + eps)\n\n    gamma = tl.load(gamma_ptr + offs_e, mask=offs_e < E, other=0.0).to(tl.float32)\n    beta = tl.load(beta_ptr + offs_e, mask=offs_e < E, other=0.0).to(tl.float32)\n\n    y = diff * inv_std\n    y = y * gamma + beta\n    tl.store(y_ptr + m * stride_y_m + offs_e * stride_y_e, y.to(y_ptr.dtype.element_ty), mask=mask)\n\n\n# ------------------------------------------------------------\n# Public API: kernel_function\n# ------------------------------------------------------------\ndef kernel_function(\n    x,\n    embed_dim=None,\n    num_heads=None,\n    in_proj_weight=None,\n    in_proj_bias=None,\n    out_proj_weight=None,\n    out_proj_bias=None,\n    norm_weight=None,\n    norm_bias=None,\n    eps: float = 1e-5,\n):\n    # Validate inputs\n    assert isinstance(x, torch.Tensor) and x.is_cuda, \"x must be a CUDA tensor\"\n    device = x.device\n    dtype = x.dtype\n\n    if embed_dim is None:\n        embed_dim = x.shape[1]\n    if num_heads is None:\n        num_heads = 1\n\n    B, C, H_img, W_img = x.shape\n    E = embed_dim\n    assert C == E, f\"C ({C}) must equal embed_dim ({E})\"\n    assert E % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n    Dh = E // num_heads\n    L = H_img * W_img\n    N = B\n    M2 = L * N\n    NH = N * num_heads\n\n    # Initialize weights if not provided (test will pass weights; this is fallback)\n    def maybe_init_weights():\n        nonlocal in_proj_weight, in_proj_bias, out_proj_weight, out_proj_bias, norm_weight, norm_bias\n        gen = torch.Generator(device=device)\n        gen.manual_seed(2025)\n        scale = 0.05\n        if in_proj_weight is None:\n            in_proj_weight = (torch.randn(3 * E, E, device=device, dtype=dtype, generator=gen) * scale).contiguous()\n        if in_proj_bias is None:\n            in_proj_bias = (torch.randn(3 * E, device=device, dtype=dtype, generator=gen) * scale).contiguous()\n        if out_proj_weight is None:\n            out_proj_weight = (torch.randn(E, E, device=device, dtype=dtype, generator=gen) * scale).contiguous()\n        if out_proj_bias is None:\n            out_proj_bias = (torch.randn(E, device=device, dtype=dtype, generator=gen) * scale).contiguous()\n        if norm_weight is None:\n            norm_weight = torch.ones(E, device=device, dtype=dtype).contiguous()\n        if norm_bias is None:\n            norm_bias = torch.zeros(E, device=device, dtype=dtype).contiguous()\n\n    maybe_init_weights()\n\n    # Weight sanity\n    assert in_proj_weight.shape == (3 * E, E)\n    assert in_proj_bias.shape == (3 * E,)\n    assert out_proj_weight.shape == (E, E)\n    assert out_proj_bias.shape == (E,)\n    assert norm_weight.shape == (E,)\n    assert norm_bias.shape == (E,)\n    for w in [in_proj_weight, in_proj_bias, out_proj_weight, out_proj_bias, norm_weight, norm_bias]:\n        assert w.is_cuda, \"All weights must reside on CUDA\"\n\n    # Views only (no compute)\n    x_lne = x.view(B, C, L).permute(2, 0, 1).contiguous()   # (L, N, E)\n    x_rows = x_lne.view(M2, E).contiguous()                 # (M2, E)\n\n    # 1) In-projection: (M2, E) x (3E, E)^T -> (M2, 3E)\n    qkv = torch.empty((M2, 3 * E), device=device, dtype=dtype)\n    BLOCK_M, BLOCK_N, BLOCK_K = 128, 128, 32\n    grid_inproj = (triton.cdiv(M2, BLOCK_M), triton.cdiv(3 * E, BLOCK_N))\n    _matmul_linear_kernel[grid_inproj](\n        x_rows, in_proj_weight, qkv, in_proj_bias,\n        M2, 3 * E, E,\n        x_rows.stride(0), x_rows.stride(1),\n        in_proj_weight.stride(0), in_proj_weight.stride(1),\n        qkv.stride(0), qkv.stride(1),\n        ADD_BIAS=True,\n        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K,\n    )\n\n    # 2) Split Q,K,V to [NH, L, Dh]\n    Q = torch.empty((NH, L, Dh), device=device, dtype=dtype)\n    K = torch.empty((NH, L, Dh), device=device, dtype=dtype)\n    V = torch.empty((NH, L, Dh), device=device, dtype=dtype)\n    grid_split = (M2,)\n    _split_qkv_kernel[grid_split](\n        qkv, Q, K, V,\n        M2, L,\n        qkv.stride(0), qkv.stride(1),\n        Q.stride(0), Q.stride(1), Q.stride(2),\n        K.stride(0), K.stride(1), K.stride(2),\n        V.stride(0), V.stride(1), V.stride(2),\n        H=num_heads, E=E, Dh=Dh,\n    )\n\n    # 3) Attention forward\n    O = torch.empty_like(Q)\n    BLOCK_M_ATT = 64\n    BLOCK_N_ATT = 64\n    scale = 1.0 / (float(Dh) ** 0.5)\n    grid_attn = (triton.cdiv(L, BLOCK_M_ATT), NH)\n    _flash_attn_fwd_kernel[grid_attn](\n        Q, K, V, O,\n        L, scale,\n        Q.stride(0), Q.stride(1), Q.stride(2),\n        K.stride(0), K.stride(1), K.stride(2),\n        V.stride(0), V.stride(1), V.stride(2),\n        O.stride(0), O.stride(1), O.stride(2),\n        BLOCK_M=BLOCK_M_ATT,\n        BLOCK_N=BLOCK_N_ATT,\n        Dh=Dh,\n    )\n\n    # 4) Merge heads to (M2, E)\n    merged = torch.empty((M2, E), device=device, dtype=dtype)\n    grid_merge = (M2,)\n    _merge_heads_kernel[grid_merge](\n        O, merged,\n        M2, L,\n        O.stride(0), O.stride(1), O.stride(2),\n        merged.stride(0), merged.stride(1),\n        H=num_heads, Dh=Dh, E=E,\n    )\n\n    # 5) Out-projection: (M2, E) x (E, E)^T -> (M2, E)\n    out_proj = torch.empty((M2, E), device=device, dtype=dtype)\n    grid_outproj = (triton.cdiv(M2, BLOCK_M), triton.cdiv(E, BLOCK_N))\n    _matmul_linear_kernel[grid_outproj](\n        merged, out_proj_weight, out_proj, out_proj_bias,\n        M2, E, E,\n        merged.stride(0), merged.stride(1),\n        out_proj_weight.stride(0), out_proj_weight.stride(1),\n        out_proj.stride(0), out_proj.stride(1),\n        ADD_BIAS=True,\n        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K,\n    )\n\n    # 6) Residual + LayerNorm\n    y_rows = torch.empty((M2, E), device=device, dtype=dtype)\n    grid_ln = (M2,)\n    _residual_layernorm_kernel[grid_ln](\n        out_proj, x_rows, y_rows,\n        norm_weight, norm_bias,\n        M2, E, eps,\n        out_proj.stride(0), out_proj.stride(1),\n        x_rows.stride(0), x_rows.stride(1),\n        y_rows.stride(0), y_rows.stride(1),\n        BLOCK_E=E,\n    )\n\n    # Reshape back to (B, C, H, W)\n    y = y_rows.view(L, N, E).permute(1, 2, 0).contiguous().view(B, C, H_img, W_img)\n    return y",
      "stdout": "Call attempt 'x, embed_dim, num_heads, all weights' failed with exception: at 29:16:\n\n    # Load full E-sized slices for q, k, v\n    offs_e = tl.arange(0, E)\n    base = qkv_ptr + m * stride_qkv_m\n    q_vec = tl.load(base + (0 * E + offs_e) * stride_qkv_n, mask=in_bounds, other=0.0)\n    k_vec = tl.load(base + (1 * E + offs_e) * stride_qkv_n, mask=in_bounds, other=0.0)\n    v_vec = tl.load(base + (2 * E + offs_e) * stride_qkv_n, mask=in_bounds, other=0.0)\n\n    d_offs = tl.arange(0, Dh)\n    for h in range(0, H):\n        idx = h * Dh + d_offs\n        q_seg = q_vec[idx]\n                ^\nDid you forget to add @triton.jit ? (`_semantic` argument must be provided outside of JIT functions.)\nCall attempt 'x, embed_dim, num_heads' failed with exception: at 29:16:\n\n    # Load full E-sized slices for q, k, v\n    offs_e = tl.arange(0, E)\n    base = qkv_ptr + m * stride_qkv_m\n    q_vec = tl.load(base + (0 * E + offs_e) * stride_qkv_n, mask=in_bounds, other=0.0)\n    k_vec = tl.load(base + (1 * E + offs_e) * stride_qkv_n, mask=in_bounds, other=0.0)\n    v_vec = tl.load(base + (2 * E + offs_e) * stride_qkv_n, mask=in_bounds, other=0.0)\n\n    d_offs = tl.arange(0, Dh)\n    for h in range(0, H):\n        idx = h * Dh + d_offs\n        q_seg = q_vec[idx]\n                ^\nDid you forget to add @triton.jit ? (`_semantic` argument must be provided outside of JIT functions.)\nCall attempt 'x only' failed with exception: at 29:16:\n\n    # Load full E-sized slices for q, k, v\n    offs_e = tl.arange(0, E)\n    base = qkv_ptr + m * stride_qkv_m\n    q_vec = tl.load(base + (0 * E + offs_e) * stride_qkv_n, mask=in_bounds, other=0.0)\n    k_vec = tl.load(base + (1 * E + offs_e) * stride_qkv_n, mask=in_bounds, other=0.0)\n    v_vec = tl.load(base + (2 * E + offs_e) * stride_qkv_n, mask=in_bounds, other=0.0)\n\n    d_offs = tl.arange(0, Dh)\n    for h in range(0, H):\n        idx = h * Dh + d_offs\n        q_seg = q_vec[idx]\n                ^\nDid you forget to add @triton.jit ? (`_semantic` argument must be provided outside of JIT functions.)\nAll calling attempts for kernel_function failed. Please ensure kernel_function accepts one of the tested signatures.\n",
      "stderr": ""
    },
    {
      "round": 3,
      "timestamp": "2025-12-03T11:31:43.424027",
      "success": false,
      "kernel_code": "import torch\nimport triton\nimport triton.language as tl\n\n# ------------------------------------------------------------\n# Matmul + optional bias: C[M,N] = A[M,K] @ B[N,K]^T + bias[N]\n# A is [M,K], B is [N,K] in memory; we tile B as (K, BN) to compute B^T\n# ------------------------------------------------------------\n@triton.jit\ndef _matmul_linear_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bn, stride_bk,\n    stride_cm, stride_cn,\n    ADD_BIAS: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n):\n    pid_m = tl.program_id(axis=0)\n    pid_n = tl.program_id(axis=1)\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n\n    a_ptrs = a_ptr + (offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_n[None, :] * stride_bn + offs_k[:, None] * stride_bk)\n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\n    k_iter = tl.cdiv(K, BLOCK_K)\n    for ki in tl.range(0, k_iter):\n        k_base = ki * BLOCK_K\n        mask_a = (offs_m[:, None] < M) & (k_base + offs_k[None, :] < K)\n        mask_b = (offs_n[None, :] < N) & (k_base + offs_k[:, None] < K)\n        a = tl.load(a_ptrs, mask=mask_a, other=0.0)\n        b = tl.load(b_ptrs, mask=mask_b, other=0.0)\n        acc = tl.dot(a, b, acc)  # accumulate in fp32\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n\n    if ADD_BIAS:\n        bias = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0).to(tl.float32)\n        acc += bias[None, :]\n\n    c = acc.to(c_ptr.dtype.element_ty)\n    c_ptrs = c_ptr + (offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn)\n    c_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n\n\n# ------------------------------------------------------------\n# Split QKV: qkv[M2, 3E] -> Q/K/V[NH, L, Dh]\n# Avoid advanced indexing on tl.tensors to remain within JIT-supported semantics.\n# ------------------------------------------------------------\n@triton.jit\ndef _split_qkv_kernel(\n    qkv_ptr,\n    Q_ptr, K_ptr, V_ptr,\n    M2, L,\n    stride_qkv_m, stride_qkv_n,\n    stride_q_h, stride_q_l, stride_q_d,\n    stride_k_h, stride_k_l, stride_k_d,\n    stride_v_h, stride_v_l, stride_v_d,\n    H: tl.constexpr,\n    E: tl.constexpr,\n    Dh: tl.constexpr,\n):\n    m = tl.program_id(axis=0)\n    if m >= M2:\n        return\n\n    # Compute (n, l) from m\n    n = m // L\n    l = m - n * L\n\n    d_offs = tl.arange(0, Dh)\n    d_mask = d_offs < Dh  # always true; keeps shapes consistent\n\n    # Each head takes a contiguous [Dh] slice inside the E dimension\n    for h in range(0, H):\n        e_start = h * Dh\n\n        # Load Q segment\n        q_src_ptrs = qkv_ptr + m * stride_qkv_m + (0 * E + e_start + d_offs) * stride_qkv_n\n        q_seg = tl.load(q_src_ptrs, mask=d_mask, other=0.0)\n\n        # Load K segment\n        k_src_ptrs = qkv_ptr + m * stride_qkv_m + (1 * E + e_start + d_offs) * stride_qkv_n\n        k_seg = tl.load(k_src_ptrs, mask=d_mask, other=0.0)\n\n        # Load V segment\n        v_src_ptrs = qkv_ptr + m * stride_qkv_m + (2 * E + e_start + d_offs) * stride_qkv_n\n        v_seg = tl.load(v_src_ptrs, mask=d_mask, other=0.0)\n\n        nh = n * H + h\n\n        q_dst_ptrs = Q_ptr + nh * stride_q_h + l * stride_q_l + d_offs * stride_q_d\n        k_dst_ptrs = K_ptr + nh * stride_k_h + l * stride_k_l + d_offs * stride_k_d\n        v_dst_ptrs = V_ptr + nh * stride_v_h + l * stride_v_l + d_offs * stride_v_d\n\n        tl.store(q_dst_ptrs, q_seg, mask=d_mask)\n        tl.store(k_dst_ptrs, k_seg, mask=d_mask)\n        tl.store(v_dst_ptrs, v_seg, mask=d_mask)\n\n\n# ------------------------------------------------------------\n# FlashAttention-like forward: O[NH, L, Dh] from Q/K/V[NH, L, Dh]\n# Online softmax, numerically stable, no causal mask\n# ------------------------------------------------------------\n@triton.jit\ndef _flash_attn_fwd_kernel(\n    Q_ptr, K_ptr, V_ptr, O_ptr,\n    L, scale,\n    stride_q_h, stride_q_l, stride_q_d,\n    stride_k_h, stride_k_l, stride_k_d,\n    stride_v_h, stride_v_l, stride_v_d,\n    stride_o_h, stride_o_l, stride_o_d,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    Dh: tl.constexpr,\n):\n    pid_m = tl.program_id(axis=0)  # sequence block id\n    pid_h = tl.program_id(axis=1)  # head id in NH\n\n    start_m = pid_m * BLOCK_M\n    offs_m = start_m + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, Dh)\n\n    # Load Q [BM, Dh]\n    q_ptrs = Q_ptr + pid_h * stride_q_h + (offs_m[:, None] * stride_q_l + offs_d[None, :] * stride_q_d)\n    q_mask = (offs_m[:, None] < L)\n    q = tl.load(q_ptrs, mask=q_mask, other=0.0)\n\n    # Online softmax state\n    m_i = tl.zeros((BLOCK_M,), dtype=tl.float32) - float(\"inf\")\n    l_i = tl.zeros((BLOCK_M,), dtype=tl.float32)\n    acc = tl.zeros((BLOCK_M, Dh), dtype=tl.float32)\n\n    n_blocks = tl.cdiv(L, BLOCK_N)\n    for nb in tl.range(0, n_blocks):\n        start_n = nb * BLOCK_N\n        offsn = start_n + offs_n\n\n        # K tile (Dh, BN)\n        k_ptrs = K_ptr + pid_h * stride_k_h + (offsn[None, :] * stride_k_l + offs_d[:, None] * stride_k_d)\n        k_mask = (offsn[None, :] < L)\n        k = tl.load(k_ptrs, mask=k_mask, other=0.0)\n\n        # qk [BM, BN]\n        qk = tl.dot(q, k).to(tl.float32)\n        qk = qk * scale\n\n        # online softmax update\n        m_ij = tl.maximum(m_i, tl.max(qk, 1))\n        p = tl.exp(qk - m_ij[:, None])\n        alpha = tl.exp(m_i - m_ij)\n        l_ij = tl.sum(p, 1)\n\n        # V tile (BN, Dh)\n        v_ptrs = V_ptr + pid_h * stride_v_h + (offsn[:, None] * stride_v_l + offs_d[None, :] * stride_v_d)\n        v_mask = (offsn[:, None] < L)\n        v = tl.load(v_ptrs, mask=v_mask, other=0.0).to(tl.float32)\n\n        acc = acc * alpha[:, None] + tl.dot(p.to(tl.float32), v)\n        l_i = l_i * alpha + l_ij\n        m_i = m_ij\n\n    out = acc * (1.0 / l_i)[:, None]\n    o_ptrs = O_ptr + pid_h * stride_o_h + (offs_m[:, None] * stride_o_l + offs_d[None, :] * stride_o_d)\n    o_mask = (offs_m[:, None] < L)\n    tl.store(o_ptrs, out.to(O_ptr.dtype.element_ty), mask=o_mask)\n\n\n# ------------------------------------------------------------\n# Merge heads: O[NH,L,Dh] -> merged[M2,E] with M2=L*N, E=H*Dh\n# ------------------------------------------------------------\n@triton.jit\ndef _merge_heads_kernel(\n    O_ptr, merged_ptr,\n    M2, L,\n    stride_o_h, stride_o_l, stride_o_d,\n    stride_m_m, stride_m_e,\n    H: tl.constexpr,\n    Dh: tl.constexpr,\n    E: tl.constexpr,\n):\n    m = tl.program_id(axis=0)\n    if m >= M2:\n        return\n\n    n = m // L\n    l = m - n * L\n\n    d_offs = tl.arange(0, Dh)\n    mask = d_offs < Dh\n    for h in range(0, H):\n        nh = n * H + h\n        o_ptr_row = O_ptr + nh * stride_o_h + l * stride_o_l + d_offs * stride_o_d\n        seg = tl.load(o_ptr_row, mask=mask, other=0.0)\n        out_ptr_row = merged_ptr + m * stride_m_m + (h * Dh + d_offs) * stride_m_e\n        tl.store(out_ptr_row, seg, mask=mask)\n\n\n# ------------------------------------------------------------\n# Residual + LayerNorm per row over E: y = LN(x + residual)\n# ------------------------------------------------------------\n@triton.jit\ndef _residual_layernorm_kernel(\n    x_ptr, residual_ptr, y_ptr,\n    gamma_ptr, beta_ptr,\n    M2, E, eps,\n    stride_x_m, stride_x_e,\n    stride_r_m, stride_r_e,\n    stride_y_m, stride_y_e,\n    BLOCK_E: tl.constexpr,\n):\n    m = tl.program_id(axis=0)\n    offs_e = tl.arange(0, BLOCK_E)\n    mask = (m < M2) & (offs_e < E)\n\n    x_row = tl.load(x_ptr + m * stride_x_m + offs_e * stride_x_e, mask=mask, other=0.0).to(tl.float32)\n    r_row = tl.load(residual_ptr + m * stride_r_m + offs_e * stride_r_e, mask=mask, other=0.0).to(tl.float32)\n    v = x_row + r_row\n\n    mean = tl.sum(v, axis=0) / E\n    diff = v - mean\n    var = tl.sum(diff * diff, axis=0) / E\n    inv_std = 1.0 / tl.sqrt(var + eps)\n\n    gamma = tl.load(gamma_ptr + offs_e, mask=offs_e < E, other=0.0).to(tl.float32)\n    beta = tl.load(beta_ptr + offs_e, mask=offs_e < E, other=0.0).to(tl.float32)\n\n    y = diff * inv_std\n    y = y * gamma + beta\n    tl.store(y_ptr + m * stride_y_m + offs_e * stride_y_e, y.to(y_ptr.dtype.element_ty), mask=mask)\n\n\n# ------------------------------------------------------------\n# Public API: kernel_function\n# Notes on fusion:\n# - We keep distinct kernels for matmul projections, attention, and LN.\n#   A single monolithic kernel would exceed register/SRAM budgets for 128*128*128 and reduce occupancy.\n#   We fuse bias into matmuls and compute residual+LN in a single kernel to minimize traffic.\n# ------------------------------------------------------------\ndef kernel_function(\n    x,\n    embed_dim=None,\n    num_heads=None,\n    in_proj_weight=None,\n    in_proj_bias=None,\n    out_proj_weight=None,\n    out_proj_bias=None,\n    norm_weight=None,\n    norm_bias=None,\n    eps: float = 1e-5,\n):\n    assert isinstance(x, torch.Tensor) and x.is_cuda, \"x must be a CUDA tensor\"\n    device = x.device\n    dtype = x.dtype\n\n    if embed_dim is None:\n        embed_dim = x.shape[1]\n    if num_heads is None:\n        num_heads = 1\n\n    B, C, H_img, W_img = x.shape\n    E = embed_dim\n    assert C == E, f\"C ({C}) must equal embed_dim ({E})\"\n    assert E % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n    Dh = E // num_heads\n    L = H_img * W_img\n    N = B\n    M2 = L * N\n    NH = N * num_heads\n\n    # Initialize weights if not provided\n    def maybe_init_weights():\n        nonlocal in_proj_weight, in_proj_bias, out_proj_weight, out_proj_bias, norm_weight, norm_bias\n        gen = torch.Generator(device=device)\n        gen.manual_seed(2025)\n        scale = 0.05\n        if in_proj_weight is None:\n            in_proj_weight = (torch.randn(3 * E, E, device=device, dtype=dtype, generator=gen) * scale).contiguous()\n        if in_proj_bias is None:\n            in_proj_bias = (torch.randn(3 * E, device=device, dtype=dtype, generator=gen) * scale).contiguous()\n        if out_proj_weight is None:\n            out_proj_weight = (torch.randn(E, E, device=device, dtype=dtype, generator=gen) * scale).contiguous()\n        if out_proj_bias is None:\n            out_proj_bias = (torch.randn(E, device=device, dtype=dtype, generator=gen) * scale).contiguous()\n        if norm_weight is None:\n            norm_weight = torch.ones(E, device=device, dtype=dtype).contiguous()\n        if norm_bias is None:\n            norm_bias = torch.zeros(E, device=device, dtype=dtype).contiguous()\n\n    maybe_init_weights()\n\n    # Weight sanity checks\n    assert in_proj_weight.shape == (3 * E, E)\n    assert in_proj_bias.shape == (3 * E,)\n    assert out_proj_weight.shape == (E, E)\n    assert out_proj_bias.shape == (E,)\n    assert norm_weight.shape == (E,)\n    assert norm_bias.shape == (E,)\n    for w in [in_proj_weight, in_proj_bias, out_proj_weight, out_proj_bias, norm_weight, norm_bias]:\n        assert w.is_cuda, \"All weights must reside on CUDA\"\n\n    # Reshape to (L, N, E) without compute\n    x_lne = x.view(B, C, L).permute(2, 0, 1).contiguous()  # (L, N, E)\n    x_rows = x_lne.view(M2, E).contiguous()                # (M2, E)\n\n    # 1) In-projection: (M2, E) x (3E, E)^T -> (M2, 3E)\n    qkv = torch.empty((M2, 3 * E), device=device, dtype=dtype)\n    BLOCK_M, BLOCK_N, BLOCK_K = 128, 128, 32\n    grid_inproj = (triton.cdiv(M2, BLOCK_M), triton.cdiv(3 * E, BLOCK_N))\n    _matmul_linear_kernel[grid_inproj](\n        x_rows, in_proj_weight, qkv, in_proj_bias,\n        M2, 3 * E, E,\n        x_rows.stride(0), x_rows.stride(1),\n        in_proj_weight.stride(0), in_proj_weight.stride(1),\n        qkv.stride(0), qkv.stride(1),\n        ADD_BIAS=True,\n        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K,\n    )\n\n    # 2) Split Q,K,V to [NH, L, Dh]\n    Q = torch.empty((NH, L, Dh), device=device, dtype=dtype)\n    K = torch.empty((NH, L, Dh), device=device, dtype=dtype)\n    V = torch.empty((NH, L, Dh), device=device, dtype=dtype)\n    grid_split = (M2,)\n    _split_qkv_kernel[grid_split](\n        qkv, Q, K, V,\n        M2, L,\n        qkv.stride(0), qkv.stride(1),\n        Q.stride(0), Q.stride(1), Q.stride(2),\n        K.stride(0), K.stride(1), K.stride(2),\n        V.stride(0), V.stride(1), V.stride(2),\n        H=num_heads, E=E, Dh=Dh,\n    )\n\n    # 3) Attention forward\n    O = torch.empty_like(Q)\n    BLOCK_M_ATT = 64\n    BLOCK_N_ATT = 64\n    scale = 1.0 / (float(Dh) ** 0.5)\n    grid_attn = (triton.cdiv(L, BLOCK_M_ATT), NH)\n    _flash_attn_fwd_kernel[grid_attn](\n        Q, K, V, O,\n        L, scale,\n        Q.stride(0), Q.stride(1), Q.stride(2),\n        K.stride(0), K.stride(1), K.stride(2),\n        V.stride(0), V.stride(1), V.stride(2),\n        O.stride(0), O.stride(1), O.stride(2),\n        BLOCK_M=BLOCK_M_ATT,\n        BLOCK_N=BLOCK_N_ATT,\n        Dh=Dh,\n    )\n\n    # 4) Merge heads to (M2, E)\n    merged = torch.empty((M2, E), device=device, dtype=dtype)\n    grid_merge = (M2,)\n    _merge_heads_kernel[grid_merge](\n        O, merged,\n        M2, L,\n        O.stride(0), O.stride(1), O.stride(2),\n        merged.stride(0), merged.stride(1),\n        H=num_heads, Dh=Dh, E=E,\n    )\n\n    # 5) Out-projection: (M2, E) x (E, E)^T -> (M2, E)\n    out_proj = torch.empty((M2, E), device=device, dtype=dtype)\n    grid_outproj = (triton.cdiv(M2, BLOCK_M), triton.cdiv(E, BLOCK_N))\n    _matmul_linear_kernel[grid_outproj](\n        merged, out_proj_weight, out_proj, out_proj_bias,\n        M2, E, E,\n        merged.stride(0), merged.stride(1),\n        out_proj_weight.stride(0), out_proj_weight.stride(1),\n        out_proj.stride(0), out_proj.stride(1),\n        ADD_BIAS=True,\n        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K,\n    )\n\n    # 6) Residual + LayerNorm\n    y_rows = torch.empty((M2, E), device=device, dtype=dtype)\n    grid_ln = (M2,)\n    _residual_layernorm_kernel[grid_ln](\n        out_proj, x_rows, y_rows,\n        norm_weight, norm_bias,\n        M2, E, eps,\n        out_proj.stride(0), out_proj.stride(1),\n        x_rows.stride(0), x_rows.stride(1),\n        y_rows.stride(0), y_rows.stride(1),\n        BLOCK_E=E,\n    )\n\n    # Reshape back to (B, C, H, W)\n    y = y_rows.view(L, N, E).permute(1, 2, 0).contiguous().view(B, C, H_img, W_img)\n    return y",
      "stdout": "NUMERICAL MISMATCH:\nCall signature used: x, embed_dim, num_heads, all weights\nInput shape: torch.Size([2, 128, 128, 128]), dtype: torch.bfloat16, device: cuda:0\nExpected shape: torch.Size([2, 128, 128, 128]), dtype: torch.bfloat16, device: cuda:0\nResult shape: torch.Size([2, 128, 128, 128]), dtype: torch.bfloat16, device: cuda:0\nTolerances used (bf16): rtol=0.01, atol=0.02\nExpected (first 10): tensor([0.4609, 0.1416, 0.8125, 0.6328, 0.1729, 0.3086, 0.5234, 0.7344, 0.7422,\n        0.3555], grad_fn=<ToCopyBackward0>)\nGot (first 10): tensor([0.4531, 0.1387, 0.8047, 0.6289, 0.1660, 0.3047, 0.5195, 0.7305, 0.7383,\n        0.3477])\nMax absolute difference: 0.03125\nMax relative error: 58771.91015625\n",
      "stderr": ""
    },
    {
      "round": 4,
      "timestamp": "2025-12-03T11:34:27.189547",
      "success": true,
      "kernel_code": "import torch\nimport triton\nimport triton.language as tl\n\n\n# ------------------------------------------------------------\n# Matmul + optional bias: C[M,N] = A[M,K] @ B[N,K]^T + bias[N]\n# A is [M,K], B is [N,K] in memory; we tile B as (K, BN) to compute B^T\n# ------------------------------------------------------------\n@triton.jit\ndef _matmul_linear_kernel(\n    a_ptr, b_ptr, c_ptr, bias_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bn, stride_bk,\n    stride_cm, stride_cn,\n    ADD_BIAS: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n):\n    pid_m = tl.program_id(axis=0)\n    pid_n = tl.program_id(axis=1)\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n\n    a_ptrs = a_ptr + (offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_n[None, :] * stride_bn + offs_k[:, None] * stride_bk)\n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    k_iter = tl.cdiv(K, BLOCK_K)\n    for ki in tl.range(0, k_iter):\n        k_base = ki * BLOCK_K\n        mask_a = (offs_m[:, None] < M) & (k_base + offs_k[None, :] < K)\n        mask_b = (offs_n[None, :] < N) & (k_base + offs_k[:, None] < K)\n        a = tl.load(a_ptrs, mask=mask_a, other=0.0)\n        b = tl.load(b_ptrs, mask=mask_b, other=0.0)\n        acc = tl.dot(a, b, acc)\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n\n    if ADD_BIAS:\n        bias = tl.load(bias_ptr + offs_n, mask=offs_n < N, other=0.0).to(tl.float32)\n        acc += bias[None, :]\n\n    c = acc.to(c_ptr.dtype.element_ty)\n    c_ptrs = c_ptr + (offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn)\n    c_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n\n\n# ------------------------------------------------------------\n# Split QKV: qkv[M2, 3E] -> Q/K/V[NH, L, Dh]\n# Correctly map flattened row index m -> (l, n) with:\n#   l = m // N, n = m % N\n# ------------------------------------------------------------\n@triton.jit\ndef _split_qkv_kernel(\n    qkv_ptr,\n    Q_ptr, K_ptr, V_ptr,\n    M2, L, N,\n    stride_qkv_m, stride_qkv_n,\n    stride_q_h, stride_q_l, stride_q_d,\n    stride_k_h, stride_k_l, stride_k_d,\n    stride_v_h, stride_v_l, stride_v_d,\n    H: tl.constexpr,\n    E: tl.constexpr,\n    Dh: tl.constexpr,\n):\n    m = tl.program_id(axis=0)\n    if m >= M2:\n        return\n\n    # Correct mapping from flattened (L,N) -> (l,n)\n    l = m // N\n    n = m - l * N  # n = m % N\n\n    d_offs = tl.arange(0, Dh)\n    d_mask = d_offs < Dh\n\n    base_m = qkv_ptr + m * stride_qkv_m\n    for h in range(0, H):\n        e_start = h * Dh\n        # Q slice\n        q_src = base_m + (0 * E + e_start + d_offs) * stride_qkv_n\n        q_seg = tl.load(q_src, mask=d_mask, other=0.0)\n        # K slice\n        k_src = base_m + (1 * E + e_start + d_offs) * stride_qkv_n\n        k_seg = tl.load(k_src, mask=d_mask, other=0.0)\n        # V slice\n        v_src = base_m + (2 * E + e_start + d_offs) * stride_qkv_n\n        v_seg = tl.load(v_src, mask=d_mask, other=0.0)\n\n        nh = n * H + h\n        q_dst = Q_ptr + nh * stride_q_h + l * stride_q_l + d_offs * stride_q_d\n        k_dst = K_ptr + nh * stride_k_h + l * stride_k_l + d_offs * stride_k_d\n        v_dst = V_ptr + nh * stride_v_h + l * stride_v_l + d_offs * stride_v_d\n        tl.store(q_dst, q_seg, mask=d_mask)\n        tl.store(k_dst, k_seg, mask=d_mask)\n        tl.store(v_dst, v_seg, mask=d_mask)\n\n\n# ------------------------------------------------------------\n# FlashAttention-like forward: O[NH, L, Dh] from Q/K/V[NH, L, Dh]\n# Online softmax, numerically stable, no causal mask\n# ------------------------------------------------------------\n@triton.jit\ndef _flash_attn_fwd_kernel(\n    Q_ptr, K_ptr, V_ptr, O_ptr,\n    L, scale,\n    stride_q_h, stride_q_l, stride_q_d,\n    stride_k_h, stride_k_l, stride_k_d,\n    stride_v_h, stride_v_l, stride_v_d,\n    stride_o_h, stride_o_l, stride_o_d,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    Dh: tl.constexpr,\n):\n    pid_m = tl.program_id(axis=0)  # sequence block id\n    pid_h = tl.program_id(axis=1)  # head id in NH\n\n    start_m = pid_m * BLOCK_M\n    offs_m = start_m + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, Dh)\n\n    # Load Q [BM, Dh]\n    q_ptrs = Q_ptr + pid_h * stride_q_h + (offs_m[:, None] * stride_q_l + offs_d[None, :] * stride_q_d)\n    q_mask = (offs_m[:, None] < L)\n    q = tl.load(q_ptrs, mask=q_mask, other=0.0)\n\n    # Online softmax state\n    m_i = tl.zeros((BLOCK_M,), dtype=tl.float32) - float(\"inf\")\n    l_i = tl.zeros((BLOCK_M,), dtype=tl.float32)\n    acc = tl.zeros((BLOCK_M, Dh), dtype=tl.float32)\n\n    n_blocks = tl.cdiv(L, BLOCK_N)\n    for nb in tl.range(0, n_blocks):\n        start_n = nb * BLOCK_N\n        offsn = start_n + offs_n\n\n        # K tile (Dh, BN)\n        k_ptrs = K_ptr + pid_h * stride_k_h + (offsn[None, :] * stride_k_l + offs_d[:, None] * stride_k_d)\n        k_mask = (offsn[None, :] < L)\n        k = tl.load(k_ptrs, mask=k_mask, other=0.0)\n\n        # qk [BM, BN]\n        qk = tl.dot(q, k).to(tl.float32)\n        qk = qk * scale\n\n        # Online softmax update\n        m_ij = tl.maximum(m_i, tl.max(qk, 1))\n        p = tl.exp(qk - m_ij[:, None])\n        alpha = tl.exp(m_i - m_ij)\n        l_ij = tl.sum(p, 1)\n\n        # V tile (BN, Dh)\n        v_ptrs = V_ptr + pid_h * stride_v_h + (offsn[:, None] * stride_v_l + offs_d[None, :] * stride_v_d)\n        v_mask = (offsn[:, None] < L)\n        v = tl.load(v_ptrs, mask=v_mask, other=0.0).to(tl.float32)\n\n        acc = acc * alpha[:, None] + tl.dot(p.to(tl.float32), v)\n        l_i = l_i * alpha + l_ij\n        m_i = m_ij\n\n    out = acc * (1.0 / l_i)[:, None]\n    o_ptrs = O_ptr + pid_h * stride_o_h + (offs_m[:, None] * stride_o_l + offs_d[None, :] * stride_o_d)\n    o_mask = (offs_m[:, None] < L)\n    tl.store(o_ptrs, out.to(O_ptr.dtype.element_ty), mask=o_mask)\n\n\n# ------------------------------------------------------------\n# Merge heads: O[NH,L,Dh] -> merged[M2,E] with M2=L*N, E=H*Dh\n# Correct mapping m -> (l, n): l = m // N, n = m % N\n# ------------------------------------------------------------\n@triton.jit\ndef _merge_heads_kernel(\n    O_ptr, merged_ptr,\n    M2, L, N,\n    stride_o_h, stride_o_l, stride_o_d,\n    stride_m_m, stride_m_e,\n    H: tl.constexpr,\n    Dh: tl.constexpr,\n    E: tl.constexpr,\n):\n    m = tl.program_id(axis=0)\n    if m >= M2:\n        return\n\n    l = m // N\n    n = m - l * N  # n = m % N\n\n    d_offs = tl.arange(0, Dh)\n    mask = d_offs < Dh\n    for h in range(0, H):\n        nh = n * H + h\n        o_ptr_row = O_ptr + nh * stride_o_h + l * stride_o_l + d_offs * stride_o_d\n        seg = tl.load(o_ptr_row, mask=mask, other=0.0)\n        out_ptr_row = merged_ptr + m * stride_m_m + (h * Dh + d_offs) * stride_m_e\n        tl.store(out_ptr_row, seg, mask=mask)\n\n\n# ------------------------------------------------------------\n# Residual + LayerNorm per row over E: y = LN(x + residual)\n# Accumulate statistics in fp32; cast on store.\n# ------------------------------------------------------------\n@triton.jit\ndef _residual_layernorm_kernel(\n    x_ptr, residual_ptr, y_ptr,\n    gamma_ptr, beta_ptr,\n    M2, E, eps,\n    stride_x_m, stride_x_e,\n    stride_r_m, stride_r_e,\n    stride_y_m, stride_y_e,\n    BLOCK_E: tl.constexpr,\n):\n    m = tl.program_id(axis=0)\n    offs_e = tl.arange(0, BLOCK_E)\n    mask = (m < M2) & (offs_e < E)\n\n    x_row = tl.load(x_ptr + m * stride_x_m + offs_e * stride_x_e, mask=mask, other=0.0).to(tl.float32)\n    r_row = tl.load(residual_ptr + m * stride_r_m + offs_e * stride_r_e, mask=mask, other=0.0).to(tl.float32)\n    v = x_row + r_row\n\n    mean = tl.sum(v, axis=0) / E\n    diff = v - mean\n    var = tl.sum(diff * diff, axis=0) / E\n    inv_std = 1.0 / tl.sqrt(var + eps)\n\n    gamma = tl.load(gamma_ptr + offs_e, mask=offs_e < E, other=0.0).to(tl.float32)\n    beta = tl.load(beta_ptr + offs_e, mask=offs_e < E, other=0.0).to(tl.float32)\n\n    y = diff * inv_std\n    y = y * gamma + beta\n    tl.store(y_ptr + m * stride_y_m + offs_e * stride_y_e, y.to(y_ptr.dtype.element_ty), mask=mask)\n\n\n# ------------------------------------------------------------\n# Public API: kernel_function\n# Notes on fusion:\n# - Heavy per-row fusion of residual+LN is applied.\n# - Projections and attention are separate kernels for register/SRAM and occupancy reasons at this size.\n# ------------------------------------------------------------\ndef kernel_function(\n    x,\n    embed_dim=None,\n    num_heads=None,\n    in_proj_weight=None,\n    in_proj_bias=None,\n    out_proj_weight=None,\n    out_proj_bias=None,\n    norm_weight=None,\n    norm_bias=None,\n    eps: float = 1e-5,\n):\n    assert isinstance(x, torch.Tensor) and x.is_cuda, \"x must be a CUDA tensor\"\n    device = x.device\n    dtype = x.dtype\n\n    if embed_dim is None:\n        embed_dim = x.shape[1]\n    if num_heads is None:\n        num_heads = 1\n\n    B, C, H_img, W_img = x.shape\n    E = embed_dim\n    assert C == E, f\"C ({C}) must equal embed_dim ({E})\"\n    assert E % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n    Dh = E // num_heads\n    L = H_img * W_img\n    N = B\n    M2 = L * N\n    NH = N * num_heads\n\n    # Initialize weights if not provided\n    def maybe_init_weights():\n        nonlocal in_proj_weight, in_proj_bias, out_proj_weight, out_proj_bias, norm_weight, norm_bias\n        gen = torch.Generator(device=device)\n        gen.manual_seed(2025)\n        scale = 0.05\n        if in_proj_weight is None:\n            in_proj_weight = (torch.randn(3 * E, E, device=device, dtype=dtype, generator=gen) * scale).contiguous()\n        if in_proj_bias is None:\n            in_proj_bias = (torch.randn(3 * E, device=device, dtype=dtype, generator=gen) * scale).contiguous()\n        if out_proj_weight is None:\n            out_proj_weight = (torch.randn(E, E, device=device, dtype=dtype, generator=gen) * scale).contiguous()\n        if out_proj_bias is None:\n            out_proj_bias = (torch.randn(E, device=device, dtype=dtype, generator=gen) * scale).contiguous()\n        if norm_weight is None:\n            norm_weight = torch.ones(E, device=device, dtype=dtype).contiguous()\n        if norm_bias is None:\n            norm_bias = torch.zeros(E, device=device, dtype=dtype).contiguous()\n\n    maybe_init_weights()\n\n    # Weight sanity checks\n    assert in_proj_weight.shape == (3 * E, E)\n    assert in_proj_bias.shape == (3 * E,)\n    assert out_proj_weight.shape == (E, E)\n    assert out_proj_bias.shape == (E,)\n    assert norm_weight.shape == (E,)\n    assert norm_bias.shape == (E,)\n    for w in [in_proj_weight, in_proj_bias, out_proj_weight, out_proj_bias, norm_weight, norm_bias]:\n        assert w.is_cuda, \"All weights must reside on CUDA\"\n\n    # Reshape to (L, N, E) then to (M2, E)\n    x_lne = x.view(B, C, L).permute(2, 0, 1).contiguous()  # (L, N, E)\n    x_rows = x_lne.view(M2, E).contiguous()                # (M2, E)\n\n    # 1) In-projection: (M2, E) x (3E, E)^T -> (M2, 3E)\n    qkv = torch.empty((M2, 3 * E), device=device, dtype=dtype)\n    BLOCK_M, BLOCK_N, BLOCK_K = 128, 128, 32\n    grid_inproj = (triton.cdiv(M2, BLOCK_M), triton.cdiv(3 * E, BLOCK_N))\n    _matmul_linear_kernel[grid_inproj](\n        x_rows, in_proj_weight, qkv, in_proj_bias,\n        M2, 3 * E, E,\n        x_rows.stride(0), x_rows.stride(1),\n        in_proj_weight.stride(0), in_proj_weight.stride(1),\n        qkv.stride(0), qkv.stride(1),\n        ADD_BIAS=True,\n        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K,\n    )\n\n    # 2) Split Q,K,V to [NH, L, Dh] (correct (l,n) mapping)\n    Q = torch.empty((NH, L, Dh), device=device, dtype=dtype)\n    K = torch.empty((NH, L, Dh), device=device, dtype=dtype)\n    V = torch.empty((NH, L, Dh), device=device, dtype=dtype)\n    grid_split = (M2,)\n    _split_qkv_kernel[grid_split](\n        qkv, Q, K, V,\n        M2, L, N,\n        qkv.stride(0), qkv.stride(1),\n        Q.stride(0), Q.stride(1), Q.stride(2),\n        K.stride(0), K.stride(1), K.stride(2),\n        V.stride(0), V.stride(1), V.stride(2),\n        H=num_heads, E=E, Dh=Dh,\n    )\n\n    # 3) Attention forward\n    O = torch.empty_like(Q)\n    BLOCK_M_ATT = 64\n    BLOCK_N_ATT = 64\n    scale = 1.0 / (float(Dh) ** 0.5)\n    grid_attn = (triton.cdiv(L, BLOCK_M_ATT), NH)\n    _flash_attn_fwd_kernel[grid_attn](\n        Q, K, V, O,\n        L, scale,\n        Q.stride(0), Q.stride(1), Q.stride(2),\n        K.stride(0), K.stride(1), K.stride(2),\n        V.stride(0), V.stride(1), V.stride(2),\n        O.stride(0), O.stride(1), O.stride(2),\n        BLOCK_M=BLOCK_M_ATT,\n        BLOCK_N=BLOCK_N_ATT,\n        Dh=Dh,\n    )\n\n    # 4) Merge heads to (M2, E) with correct (l,n) mapping\n    merged = torch.empty((M2, E), device=device, dtype=dtype)\n    grid_merge = (M2,)\n    _merge_heads_kernel[grid_merge](\n        O, merged,\n        M2, L, N,\n        O.stride(0), O.stride(1), O.stride(2),\n        merged.stride(0), merged.stride(1),\n        H=num_heads, Dh=Dh, E=E,\n    )\n\n    # 5) Out-projection: (M2, E) x (E, E)^T -> (M2, E)\n    out_proj = torch.empty((M2, E), device=device, dtype=dtype)\n    grid_outproj = (triton.cdiv(M2, BLOCK_M), triton.cdiv(E, BLOCK_N))\n    _matmul_linear_kernel[grid_outproj](\n        merged, out_proj_weight, out_proj, out_proj_bias,\n        M2, E, E,\n        merged.stride(0), merged.stride(1),\n        out_proj_weight.stride(0), out_proj_weight.stride(1),\n        out_proj.stride(0), out_proj.stride(1),\n        ADD_BIAS=True,\n        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K,\n    )\n\n    # 6) Residual + LayerNorm\n    y_rows = torch.empty((M2, E), device=device, dtype=dtype)\n    grid_ln = (M2,)\n    _residual_layernorm_kernel[grid_ln](\n        out_proj, x_rows, y_rows,\n        norm_weight, norm_bias,\n        M2, E, eps,\n        out_proj.stride(0), out_proj.stride(1),\n        x_rows.stride(0), x_rows.stride(1),\n        y_rows.stride(0), y_rows.stride(1),\n        BLOCK_E=E,\n    )\n\n    # Reshape back to (B, C, H, W)\n    y = y_rows.view(L, N, E).permute(1, 2, 0).contiguous().view(B, C, H_img, W_img)\n    return y",
      "stdout": "Test passed.\n",
      "stderr": ""
    }
  ]
}
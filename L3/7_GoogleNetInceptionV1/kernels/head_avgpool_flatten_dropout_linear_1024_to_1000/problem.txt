Implement a Triton kernel that computes the following subgraph end-to-end.

        Subgraph ID: head_avgpool_flatten_dropout_linear_1024_to_1000
        Type: head_fused
        Data layout: NCHW
        DType: float32

        Shapes:
        - input: [10, 1024, 7, 7]

        - output: [10, 1000]

        Weights (fused): {
  "fc.weight": [
    1000,
    1024
  ],
  "fc.bias": [
    1000
  ]
}
        Weights (original): null

        Operations in order (with parameters):
        [
  {
    "op": "adaptive_avg_pool2d",
    "output_size": [
      1,
      1
    ]
  },
  {
    "op": "flatten",
    "start_dim": 1
  },
  {
    "op": "dropout",
    "p": 0.0
  },
  {
    "op": "linear",
    "in_features": 1024,
    "out_features": 1000,
    "bias": true
  }
]

        Requirements:
        - Return a complete Python file with a @triton.jit kernel and a wrapper function named kernel_function(...).
        - kernel_function must accept input tensor(s) and any required weights/bias parameters (match shapes above).
        - Implement the exact semantics of the listed ops in the given order for the provided shapes.
        - Use NCHW layout and float32 dtype semantics.
        - The test will import kernel_function and compare to the reference implementation below.

        Test tolerance policy (enforced in generated tests):
        - Default tolerances: rtol=1e-3, atol=1e-3.
        - Absolute cap: NEVER exceed rtol=1e-2 or atol=1e-2 in torch.allclose.
        - For float16/bfloat16 inputs: use rtol=1e-2, atol=1e-2 at most (do not go higher).
        - Include a one-line comment if you relax from default; never exceed the cap.

        Reference PyTorch implementation (exact semantics to match):

```python
import torch
import torch.nn.functional as F

def reference(x):

    # TODO: op 'adaptive_avg_pool2d' not explicitly handled; update generator if needed
    # TODO: op 'flatten' not explicitly handled; update generator if needed
    # TODO: op 'dropout' not explicitly handled; update generator if needed
    # TODO: op 'linear' not explicitly handled; update generator if needed
    return x
```

Original source snippet (AvgPoolFlattenDropoutLinear):
```python
def forward(self, x):
    x = self.avgpool(x)
    x = torch.flatten(x, 1)
    x = self.dropout(x)
    x = self.fc(x)
    return x
```
